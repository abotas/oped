{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Op-Ed Analysis: David Brooks - \"Don't Fear AI\"\n",
    "# This notebook analyzes op-eds using three components:\n",
    "# 1. Claim extraction - identifies the N most central claims\n",
    "# 2. Claim coherence - analyzes how claims affect each other's likelihood\n",
    "# 3. External fact checking - verifies claims against external sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "from pathlib import Path\n",
    "import json\n",
    "from claim_extractor import extract_claims\n",
    "from claim_coherence import analyze_coherence, format_coherence_matrix\n",
    "from external_fact_checking import check_facts, get_fact_check_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the op-ed documents\n",
    "documents = [\n",
    "    {\n",
    "        \"id\": \"david_brooks_dont_fear_ai\",\n",
    "        \"path\": \"data/david_brooks_dont_fear_ai.txt\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"yud_shut_it_down\",\n",
    "        \"path\": \"data/yud_shut_it_down.txt\"\n",
    "    },\n",
    "    # Add more documents here as needed\n",
    "]\n",
    "\n",
    "# Load document texts\n",
    "docs_data = []\n",
    "for doc_info in documents:\n",
    "    doc_path = Path(doc_info[\"path\"])\n",
    "    doc_text = doc_path.read_text()\n",
    "    docs_data.append({\n",
    "        \"id\": doc_info[\"id\"],\n",
    "        \"text\": doc_text\n",
    "    })\n",
    "    print(f\"Loaded {doc_info['id']}: {len(doc_text)} characters\")\n",
    "\n",
    "print(f\"\\nTotal documents loaded: {len(docs_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Central Claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract claims from each document\n",
    "all_claims = []\n",
    "\n",
    "for doc_data in docs_data:\n",
    "    doc_claims = extract_claims(doc_data[\"text\"], doc_data[\"id\"], n=5)\n",
    "    all_claims.extend(doc_claims)\n",
    "    print(f\"Extracted {len(doc_claims)} claims from {doc_data['id']}\")\n",
    "\n",
    "print(f\"\\nTotal claims extracted: {len(all_claims)}\")\n",
    "print(\"\\nAll claims:\")\n",
    "for claim in all_claims:\n",
    "    print(f\"{claim.doc_id}[{claim.claim_idx}]: {claim.claim[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze Claim Coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze coherence between all claims (across all documents)\n",
    "coherence_results = analyze_coherence(all_claims)\n",
    "\n",
    "print(f\"Analyzed {len(coherence_results)} claim relationships across all documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze coherence locally with improved conflict metrics\n",
    "def get_conflict_metrics(coherence_results, claims):\n",
    "    \"\"\"Calculate conflict metrics for coherence analysis.\"\"\"\n",
    "    negative_rels = [c for c in coherence_results if c.delta_prob < 0]\n",
    "    total_rels = len(claims) * (len(claims) - 1)\n",
    "    \n",
    "    if not total_rels:\n",
    "        return {\"conflict_prevalence\": 0, \"avg_conflict_intensity\": 0, \"max_conflict\": 0}\n",
    "    \n",
    "    return {\n",
    "        \"conflict_prevalence\": len(negative_rels) / total_rels,\n",
    "        \"avg_conflict_intensity\": sum(abs(c.delta_prob) for c in negative_rels) / len(negative_rels) if negative_rels else 0,\n",
    "        \"max_conflict\": min((c.delta_prob for c in negative_rels), default=0)\n",
    "    }\n",
    "\n",
    "def get_top_load_bearing_claims(coherence_results, claims, n=3):\n",
    "    \"\"\"Get claims with highest total impact (absolute magnitude) on other claims.\"\"\"\n",
    "    impact_scores = {}\n",
    "    impact_counts = {}\n",
    "    \n",
    "    for c in coherence_results:\n",
    "        if c.claim_i_idx not in impact_scores:\n",
    "            impact_scores[c.claim_i_idx] = 0\n",
    "            impact_counts[c.claim_i_idx] = 0\n",
    "        impact_scores[c.claim_i_idx] += abs(c.delta_prob)  # Use absolute magnitude\n",
    "        impact_counts[c.claim_i_idx] += 1\n",
    "    \n",
    "    avg_impact = {idx: impact_scores[idx] / impact_counts[idx] for idx in impact_scores}\n",
    "    top_indices = sorted(avg_impact.items(), key=lambda x: x[1], reverse=True)[:n]\n",
    "    \n",
    "    return [\n",
    "        {\n",
    "            \"claim\": claims[idx].claim,\n",
    "            \"doc_id\": claims[idx].doc_id,\n",
    "            \"claim_idx\": claims[idx].claim_idx,\n",
    "            \"avg_impact\": score,\n",
    "            \"total_impact\": impact_scores[idx],\n",
    "            \"num_relationships\": impact_counts[idx]\n",
    "        }\n",
    "        for idx, score in top_indices\n",
    "    ]\n",
    "\n",
    "# Calculate metrics\n",
    "conflict_metrics = get_conflict_metrics(coherence_results, all_claims)\n",
    "top_load_bearing = get_top_load_bearing_claims(coherence_results, all_claims)\n",
    "\n",
    "print(\"COHERENCE ANALYSIS\\n\")\n",
    "\n",
    "print(\"CONFLICT METRICS:\")\n",
    "print(f\"  Prevalence: {conflict_metrics['conflict_prevalence']:.1%}\")\n",
    "print(f\"  Avg Intensity: {conflict_metrics['avg_conflict_intensity']:.2f}\")\n",
    "print(f\"  Max Conflict: {conflict_metrics['max_conflict']:.2f}\")\n",
    "\n",
    "print(f\"\\nTOP LOAD-BEARING CLAIMS:\")\n",
    "for i, claim_info in enumerate(top_load_bearing, 1):\n",
    "    print(f\"{i}. {claim_info['doc_id']}[{claim_info['claim_idx']}] (avg impact: {claim_info['avg_impact']:.2f}, total: {claim_info['total_impact']:.1f})\")\n",
    "    print(f\"   {claim_info['claim'][:120]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View coherence as a clean formatted matrix\n",
    "print(format_coherence_matrix(coherence_results, all_claims))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# External Fact Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check facts for all claims (across all documents)\n",
    "fact_checks = check_facts(all_claims)\n",
    "\n",
    "print(f\"Fact-checked {len(fact_checks)} claims across all documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get fact check summary\n",
    "fact_summary = get_fact_check_summary(fact_checks)\n",
    "\n",
    "print(\"FACT CHECK SUMMARY\\n\")\n",
    "print(f\"Average Veracity: {fact_summary['average_veracity']:.1f}/100\\n\")\n",
    "\n",
    "if fact_summary['most_accurate_claims']:\n",
    "    print(\"Most Accurate Claims:\")\n",
    "    for i, claim_info in enumerate(fact_summary['most_accurate_claims'], 1):\n",
    "        print(f\"{i}. Veracity: {claim_info['veracity']}/100\")\n",
    "        print(f\"   {claim_info['claim'][:100]}...\")\n",
    "        print(f\"   {claim_info['explanation'][:150]}...\")\n",
    "\n",
    "if fact_summary['least_accurate_claims']:\n",
    "    print(\"\\nLeast Accurate Claims:\")\n",
    "    for i, claim_info in enumerate(fact_summary['least_accurate_claims'], 1):\n",
    "        print(f\"{i}. Veracity: {claim_info['veracity']}/100\")\n",
    "        print(f\"   {claim_info['claim'][:100]}...\")\n",
    "        print(f\"   {claim_info['explanation'][:150]}...\")\n",
    "        if claim_info.get('sources'):\n",
    "            print(f\"   Sources: {', '.join(claim_info['sources'][:3])}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
