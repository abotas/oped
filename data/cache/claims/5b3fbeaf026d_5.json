[
  {
    "doc_id": "yud_shut_it_down",
    "claim_idx": 0,
    "claim": "Under current global conditions, developing and deploying superhumanly intelligent AI is most likely to cause near-term extinction of all humans and biological life on Earth, because we lack the precision, preparation, and scientific understanding required to control such systems."
  },
  {
    "doc_id": "yud_shut_it_down",
    "claim_idx": 1,
    "claim": "The open letter\u2019s call for a six-month pause on training AI systems more powerful than GPT-4 is grossly insufficient; instead, there must be an indefinite, worldwide shutdown of large AI training runs and GPU clusters, with no exceptions for governments or militaries, enforced via international agreements, GPU tracking, and, if necessary, military action against rogue datacenters."
  },
  {
    "doc_id": "yud_shut_it_down",
    "claim_idx": 2,
    "claim": "AI capabilities are advancing vastly faster than AI alignment and mechanistic understanding, there is no viable safety plan, and because failure on the first attempt with superhuman AI would be terminal, continuing development is unjustifiable."
  },
  {
    "doc_id": "yud_shut_it_down",
    "claim_idx": 3,
    "claim": "The existential danger from advanced AI does not depend on whether AIs are conscious; powerful optimizing systems are lethal regardless, and given our current inability to interpret these systems\u2019 internals, scaling to models like a GPT-5-level system risks creating possibly self-aware entities and demonstrates dangerous ignorance."
  },
  {
    "doc_id": "yud_shut_it_down",
    "claim_idx": 4,
    "claim": "The AI industry faces a collective action problem and cannot be relied upon to halt progress; therefore, governments and the broader public must intervene to impose and maintain the global moratorium, treating AI extinction risk as a shared survival issue rather than a national competition."
  }
]