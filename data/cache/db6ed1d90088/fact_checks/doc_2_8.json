{
  "doc_id": "doc_2",
  "claim_idx": 8,
  "claim": "Addressing safety is prerequisite: society must solve alignment\u2014robustly steering AI toward our collectively long-term goals\u2014before and while making superintelligence cheap, widely available, and not overly concentrated in any person, company, or country.",
  "veracity": 50,
  "explanation": "This is a normative policy claim, not a verifiable fact. Some authorities argue safety/alignment should be addressed before and alongside deployment of frontier/superintelligent AI (e.g., the U.K.-led Bletchley Declaration), and major labs publicly prioritize superalignment, supporting part of the claim. ([gov.uk](https://www.gov.uk/government/publications/ai-safety-summit-2023-the-bletchley-declaration?utm_source=chatgpt.com), [openai.com](https://openai.com/index/introducing-superalignment/?utm_source=chatgpt.com), [anthropic.com](https://www.anthropic.com/news/core-views-on-ai-safety?utm_source=chatgpt.com)) Regulators also warn about harmful concentration of market power in AI, aligning with the concern about over\u2011concentration. ([gov.uk](https://www.gov.uk/government/news/cma-outlines-growing-concerns-in-markets-for-ai-foundation-models?utm_source=chatgpt.com)) However, other experts caution that \u201csolving alignment\u201d first is speculative or distracts from urgent, demonstrable harms, and leading frameworks emphasize iterative risk management rather than a prerequisite \u201csolution\u201d before availability. ([dl.acm.org](https://dl.acm.org/doi/abs/10.1145/3442188.3445922?utm_source=chatgpt.com), [nist.gov](https://www.nist.gov/publications/artificial-intelligence-risk-management-framework-ai-rmf-10?utm_source=chatgpt.com)) Given active debate and lack of empirical testability, the claim is partly supported but not factually determinable.",
  "sources": []
}