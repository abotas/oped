[
  {
    "doc_id": "david_brooks_dont_fear_ai",
    "claim_idx": 0,
    "claim": "Artificial intelligence will not replace humans but will function as a complementary tool that makes people freer to focus on distinctively human activities in the future.",
    "veracity": 55,
    "explanation": "Partly accurate: evidence to date shows AI often complements workers and can raise productivity rather than fully automating entire jobs, at least so far. ([nber.org](https://www.nber.org/papers/w31161?utm_source=chatgpt.com), [ilo.org](https://www.ilo.org/resource/news/generative-ai-likely-augment-rather-destroy-jobs?utm_source=chatgpt.com)) However, credible forecasts also project substantial substitution and outright job losses in some roles, so the categorical \u201cwill not replace humans\u201d is overstated. ([imf.org](https://www.imf.org/en/Blogs/Articles/2024/01/14/ai-will-transform-the-global-economy-lets-make-sure-it-benefits-humanity/?utm_source=chatgpt.com), [weforum.org](https://www.weforum.org/press/2025/01/future-of-jobs-report-2025-78-million-new-job-opportunities-by-2030-but-urgent-upskilling-needed-to-prepare-workforces/?utm_source=chatgpt.com)) The IMF estimates about 60% of jobs in advanced economies are exposed to AI, with roughly half of exposed roles at risk of substitution and some disappearing. ([imf.org](https://www.imf.org/en/Blogs/Articles/2024/01/14/ai-will-transform-the-global-economy-lets-make-sure-it-benefits-humanity/?utm_source=chatgpt.com)) The World Economic Forum\u2019s 2025 report projects 92 million jobs displaced globally by 2030 (alongside larger gross job creation). ([weforum.org](https://www.weforum.org/press/2025/01/future-of-jobs-report-2025-78-million-new-job-opportunities-by-2030-but-urgent-upskilling-needed-to-prepare-workforces/?utm_source=chatgpt.com)) OECD analysis similarly finds notable automation risk and mixed wage/productivity effects. ([oecd-ilibrary.org](https://www.oecd-ilibrary.org/en/publications/oecd-employment-outlook-2023_08785bba-en/full-report/artificial-intelligence-job-quality-and-inclusiveness_a713d0ad.html?utm_source=chatgpt.com))",
    "sources": []
  },
  {
    "doc_id": "david_brooks_dont_fear_ai",
    "claim_idx": 1,
    "claim": "Human cognition is not equivalent to computation; because the human mind integrates conscious and unconscious processes, emotions, embodied wisdom, moral sentiments, agency, and unique lived experience, AI\u2014which lacks consciousness, self-awareness, emotions, and a personal worldview\u2014cannot replicate it.",
    "veracity": 55,
    "explanation": "Parts of the claim are accurate: mainstream neuroscience finds that human cognition tightly integrates emotion and other processes rather than separating \u2018cold\u2019 reason from affect. ([openurl.ebsco.com](https://openurl.ebsco.com/c/eafhff/openurl?date=2008-2&doi=10.1038%2Fnrn2317&epage=158&genre=article&issn=1471-003X&issue=2&prompt=true&sid=Primo&spage=148&title=Nature+Reviews+Neuroscience&volume=9&utm_source=chatgpt.com)) However, the stronger assertion that human cognition is not equivalent to computation is a live philosophical dispute\u2014not a settled fact\u2014with influential computational theories on one side and embodied/extended approaches challenging them on the other. ([plato.stanford.edu](https://plato.stanford.edu/archives/spr2017/entries/computational-mind/?utm_source=chatgpt.com)) It is also correct that today\u2019s AI systems are not known to be conscious or self-aware, and there is no accepted test; recent theory\u2011based checklists conclude current models do not meet consciousness indicators. ([science.org](https://www.science.org/content/article/if-ai-becomes-conscious-how-will-we-know/?utm_source=chatgpt.com), [arxiv.org](https://arxiv.org/abs/2308.08708?utm_source=chatgpt.com)) But stating AI cannot ever replicate human cognition goes beyond evidence: prominent analyses argue current systems are unlikely conscious now, yet future systems could satisfy proposed indicators. ([arxiv.org](https://arxiv.org/abs/2303.07103?utm_source=chatgpt.com))",
    "sources": []
  },
  {
    "doc_id": "david_brooks_dont_fear_ai",
    "claim_idx": 2,
    "claim": "AI is a narrower, different kind of intelligence that excels at pattern synthesis and correlations but struggles with causality, narrative reasoning, and temporal understanding, in part because it only accesses conscious language rather than embodied or implicit knowledge.",
    "veracity": 78,
    "explanation": "Mostly accurate. Research shows today\u2019s ML/LLM systems excel at associational pattern-fitting but are weak at robust causal reasoning (Pearl\u2019s ladder; most ML operates at the association level). Benchmarks also find persistent gaps in temporal reasoning and long\u2011context use, and in causal/narrative reasoning over stories. Text\u2011trained models lack grounding in embodied experience\u2014\u201ca system trained only on form has a priori no way to learn meaning.\u201d However, the claim overgeneralizes by implying AI only accesses language; modern systems are broader than text-only, so I deduct points. ([cacmb4.acm.org](https://cacmb4.acm.org/magazines/2019/3/234929-the-seven-tools-of-causal-inference-with-reflections-on-machine-learning/fulltext?utm_source=chatgpt.com), [arxiv.org](https://arxiv.org/abs/2311.17667?utm_source=chatgpt.com), [aclweb.org](https://www.aclweb.org/anthology/2020.acl-main.463.pdf))",
    "sources": []
  },
  {
    "doc_id": "david_brooks_dont_fear_ai",
    "claim_idx": 3,
    "claim": "In the near term, AI will act as an ally that boosts productivity and democratizes access to expertise and education worldwide\u2014e.g., tutoring, drafting routine communications, and disproportionately helping less-skilled users\u2014with early evidence such as a 2023 Harvard Business School/Boston Consulting Group study showing consultants produced 40% higher-quality work when using AI.",
    "veracity": 86,
    "explanation": "Mostly accurate. Strong early evidence shows genAI boosts productivity and especially helps lower\u2011skilled workers: a large field study found a 14% average productivity gain for customer\u2011support agents, with roughly 34\u201335% gains for novices. ([nber.org](https://www.nber.org/papers/w31161?utm_source=chatgpt.com), [siepr.stanford.edu](https://siepr.stanford.edu/news/generative-ai-boost-can-boost-productivity-without-replacing-workers?utm_source=chatgpt.com)) For routine writing, an RCT found time fell ~40% and quality rose ~18%. ([news.mit.edu](https://news.mit.edu/2023/study-finds-chatgpt-boosts-worker-productivity-writing-0714?utm_source=chatgpt.com)) The cited 2023 HBS/BCG experiment did find \u201cmore than 40%\u201d higher quality for consultants on tasks within AI\u2019s competence frontier (along with +12% tasks completed and 25% faster), though not for all tasks. ([hbs.edu](https://www.hbs.edu/faculty/Pages/item.aspx?num=64700)) On democratizing expertise/education, RCTs show AI\u2011assisted tutoring can raise mastery (e.g., +4 p.p. in a 900\u2011tutor study; sizable math gains via a low\u2011cost WhatsApp tutor in Ghana), but misuse can harm learning (a Wharton RCT saw lower exam scores when students relied on genAI tutors). ([arxiv.org](https://arxiv.org/abs/2410.03017?utm_source=chatgpt.com), [axios.com](https://www.axios.com/local/san-francisco/2024/08/22/ai-tutor-bay-area-classrooms?utm_source=chatgpt.com)) Overall, the claim is well\u2011grounded but slightly overgeneralizes the 40% figure beyond its tested scope.",
    "sources": []
  },
  {
    "doc_id": "david_brooks_dont_fear_ai",
    "claim_idx": 4,
    "claim": "Although bad actors will misuse AI, most people will use it for learning and innovation, leading to net positive outcomes (including medical advances), and AI\u2019s ultimate effect will be to highlight human distinctiveness and encourage greater emphasis on caregiving, teamwork, deep reading, exploration, spirituality, and enjoyment.",
    "veracity": 60,
    "explanation": "Parts of the claim are supported, but the broad, value-laden prediction is not empirically established. Evidence shows many users do adopt generative AI for learning and productivity: in 2025, 34% of U.S. adults had used ChatGPT and 26% used it for learning (\u22483/4 of users), and controlled studies/reviews find task-level productivity gains from generative AI, especially for less-experienced workers. ([pewresearch.org](https://www.pewresearch.org/short-reads/2025/06/25/34-of-us-adults-have-used-chatgpt-about-double-the-share-in-2023/?utm_source=chatgpt.com), [oecd.org](https://www.oecd.org/en/publications/oecd-employment-outlook-2023_08785bba-en/full-report/artificial-intelligence-job-quality-and-inclusiveness_a713d0ad.html?utm_source=chatgpt.com)) Medical advances are real: AlphaFold 3 expanded structure/interaction prediction with implications for drug discovery, hundreds of AI/ML medical devices have been cleared by the U.S. FDA, and an AI-designed drug reported positive Phase IIa results. ([pmc.ncbi.nlm.nih.gov](https://pmc.ncbi.nlm.nih.gov/articles/PMC11168924/?utm_source=chatgpt.com), [fda.gov](https://www.fda.gov/medical-devices/software-medical-device-samd/artificial-intelligence-and-machine-learning-aiml-enabled-medical-devices?utm_source=chatgpt.com), [insilico.com](https://insilico.com/news/tnik-ipf-phase2a?utm_source=chatgpt.com)) Misuse by bad actors is also well-documented (e.g., Europol\u2019s warnings on AI-enabled crime and the FTC\u2019s proposed rule on AI-driven impersonation). ([reuters.com](https://www.reuters.com/world/europe/europol-warns-ai-driven-crime-threats-2025-03-18/?utm_source=chatgpt.com), [ftc.gov](https://www.ftc.gov/news-events/news/press-releases/2024/02/ftc-proposes-new-protections-combat-ai-impersonation-individuals?utm_source=chatgpt.com)) As for AI ultimately elevating \u201chuman distinctiveness\u201d and shifting society toward caregiving, teamwork, deep reading, exploration, spirituality, and enjoyment: analysts do foresee rising emphasis on person\u2011facing and social skills, but this remains conditional and speculative rather than demonstrated. ([nber.org](https://www.nber.org/papers/w32140?utm_source=chatgpt.com), [jp.weforum.org](https://jp.weforum.org/publications/the-future-of-jobs-report-2025/in-full/3-skills-outlook/?utm_source=chatgpt.com))",
    "sources": []
  },
  {
    "doc_id": "yud_shut_it_down",
    "claim_idx": 0,
    "claim": "Under current global conditions, developing and deploying superhumanly intelligent AI is most likely to cause near-term extinction of all humans and biological life on Earth, because we lack the precision, preparation, and scientific understanding required to control such systems.",
    "veracity": 10,
    "explanation": "The claim overstates both likelihood and scope. A 2024 survey of 2,778 AI researchers found most think good outcomes from superhuman AI are more likely than bad, with typical (median) estimates for existential catastrophe around 5%\u2014far from \"most likely.\" ([arxiv.org](https://arxiv.org/abs/2401.02843?utm_source=chatgpt.com)) The UK government\u2019s 2024 International Scientific Report on Advanced AI Safety characterizes the outlook as highly uncertain, noting limited understanding and real risks, but it does not conclude that near\u2011term human (let alone biosphere\u2011wide) extinction is the most probable outcome. ([gov.uk](https://www.gov.uk/government/publications/international-scientific-report-on-the-safety-of-advanced-ai/international-scientific-report-on-the-safety-of-advanced-ai-interim-report?utm_source=chatgpt.com)) Moreover, Earth\u2019s worst mass extinctions eliminated many species but never \u201call biological life,\u201d making that part of the claim scientifically implausible. ([news.uchicago.edu](https://news.uchicago.edu/story/mass-extinctions-remove-species-not-ecological-variety?utm_source=chatgpt.com)) While many experts urge prioritizing mitigation of non\u2011zero existential risks, their consensus statement frames it as a priority alongside pandemics and nuclear war\u2014not as the near\u2011term dominant outcome. ([safe.ai](https://safe.ai/work/press-release-ai-risk?utm_source=chatgpt.com))",
    "sources": []
  },
  {
    "doc_id": "yud_shut_it_down",
    "claim_idx": 1,
    "claim": "The open letter\u2019s call for a six-month pause on training AI systems more powerful than GPT-4 is grossly insufficient; instead, there must be an indefinite, worldwide shutdown of large AI training runs and GPU clusters, with no exceptions for governments or militaries, enforced via international agreements, GPU tracking, and, if necessary, military action against rogue datacenters.",
    "veracity": 50,
    "explanation": "The only checkable element is accurate: the 2023 Future of Life Institute open letter did call for at least a six\u2011month pause on training AI systems more powerful than GPT\u20114. ([futureoflife.org](https://futureoflife.org/open-letter/pause-giant-ai-experiments/?utm_source=chatgpt.com)) The rest of the statement is a prescriptive opinion (that such a pause is \u201cgrossly insufficient\u201d and that there \u201cmust be\u201d an indefinite, worldwide shutdown and even military enforcement), which is not a factual claim and is unsupported by existing international AI governance. Current multilateral frameworks\u2014the UK\u2019s Bletchley Declaration, the UN General Assembly\u2019s 2024 AI resolution, and the EU AI Act\u2014focus on risk\u2011based oversight and safety evaluation rather than indefinite shutdowns or military action. ([gov.uk](https://www.gov.uk/government/publications/ai-safety-summit-2023-the-bletchley-declaration?utm_source=chatgpt.com), [press.un.org](https://press.un.org/en/2024/ga12588.doc.htm?utm_source=chatgpt.com), [europarl.europa.eu](https://www.europarl.europa.eu/news/en/press-room/20240308IPR19015/artificial-intelligence-act-meps-adopt-landmark-law?utm_source=chatgpt.com), [commission.europa.eu](https://commission.europa.eu/news/ai-act-enters-force-2024-08-01_en?utm_source=chatgpt.com))",
    "sources": []
  },
  {
    "doc_id": "yud_shut_it_down",
    "claim_idx": 2,
    "claim": "AI capabilities are advancing vastly faster than AI alignment and mechanistic understanding, there is no viable safety plan, and because failure on the first attempt with superhuman AI would be terminal, continuing development is unjustifiable.",
    "veracity": 35,
    "explanation": "Partly accurate but overstated. It is true that AI capabilities have been advancing very quickly (e.g., training compute for frontier models grew roughly 4\u20135x per year through May 2024), while deep mechanistic understanding remains limited and hard to scale. ([epoch.ai](https://epoch.ai/blog/training-compute-of-frontier-ai-models-grows-by-4-5x-per-year?utm_source=chatgpt.com)) ([arxiv.org](https://arxiv.org/abs/2404.14082?utm_source=chatgpt.com)) However, the claim that there is \u201cno viable safety plan\u201d is incorrect: major labs and governments have concrete, evolving safety frameworks and staged gating/evaluation processes (e.g., Anthropic\u2019s Responsible Scaling Policy updates; OpenAI\u2019s Preparedness Framework; the UK\u2019s AI Safety Institute evaluations). ([anthropic.com](https://www.anthropic.com/rsp-updates?utm_source=chatgpt.com), [openai.com](https://openai.com/index/updating-our-preparedness-framework/?utm_source=chatgpt.com), [aisi.gov.uk](https://www.aisi.gov.uk/work/our-first-year?utm_source=chatgpt.com)) The assertion that a single failed first attempt with superhuman AI would be terminal is speculative and not established; current evaluation results and governance approaches anticipate iterative testing and intervention rather than one-shot irreversibility. ([arxiv.org](https://arxiv.org/abs/2403.13793?utm_source=chatgpt.com))",
    "sources": []
  },
  {
    "doc_id": "yud_shut_it_down",
    "claim_idx": 3,
    "claim": "The existential danger from advanced AI does not depend on whether AIs are conscious; powerful optimizing systems are lethal regardless, and given our current inability to interpret these systems\u2019 internals, scaling to models like a GPT-5-level system risks creating possibly self-aware entities and demonstrates dangerous ignorance.",
    "veracity": 58,
    "explanation": "Partly accurate. Safety literature argues that existential risk from advanced AI stems from misaligned, power\u2011seeking optimization rather than consciousness; formal work shows optimal policies often seek power, and mesa\u2011optimization can yield misaligned objectives. ([arxiv.org](https://arxiv.org/abs/1912.01683?utm_source=chatgpt.com)) Interpretability of modern models is still limited (e.g., superposition/polysemantic features) and major standards bodies flag the need for better explainability, though there is active research progress. ([arxiv.org](https://arxiv.org/abs/2209.10652?utm_source=chatgpt.com), [nist.gov](https://www.nist.gov/itl/ai-risk-management-framework?subType=LANDING_PAGE&utm_source=chatgpt.com)) Leading labs\u2019 responsible\u2011scaling policies explicitly note that risks increase with capability, motivating stronger safeguards. ([anthropic.com](https://www.anthropic.com/news/anthropics-responsible-scaling-policy?utm_source=chatgpt.com)) However, asserting that scaling to a \u201cGPT\u20115\u2011level\u201d model risks creating self\u2011aware entities goes beyond current evidence: prominent analyses conclude no existing systems are conscious and treat future AI consciousness as uncertain\u2014not implied by scale alone. ([arxiv.org](https://arxiv.org/abs/2308.08708?utm_source=chatgpt.com))",
    "sources": []
  },
  {
    "doc_id": "yud_shut_it_down",
    "claim_idx": 4,
    "claim": "The AI industry faces a collective action problem and cannot be relied upon to halt progress; therefore, governments and the broader public must intervene to impose and maintain the global moratorium, treating AI extinction risk as a shared survival issue rather than a national competition.",
    "veracity": 50,
    "explanation": "Evidence supports that AI development has race/collective\u2011action dynamics that can discourage unilateral pauses (e.g., firms face incentives to skimp on safety to win) and thus industry self\u2011restraint is unreliable. ([arxiv.org](https://arxiv.org/abs/1907.04534?utm_source=chatgpt.com), [ora.ox.ac.uk](https://ora.ox.ac.uk/objects/uuid%3Ad87d8e34-22d6-4597-ac31-041fcb63903f?utm_source=chatgpt.com)) Calls for a moratorium have been made, but coordination faces legal/antitrust and practical enforcement hurdles, making a global, industry\u2011led halt unlikely. ([arxiv.org](https://arxiv.org/abs/2310.00374?utm_source=chatgpt.com), [brookings.edu](https://www.brookings.edu/articles/the-problems-with-a-moratorium-on-training-large-ai-systems/?utm_source=chatgpt.com)) Governments (including the UK) acknowledge potentially catastrophic cross\u2011border risks and the need for cooperation, aligning with the \u201cshared survival\u201d framing, but they have not endorsed a global moratorium. ([gov.uk](https://www.gov.uk/government/publications/ai-safety-summit-2023-the-bletchley-declaration?utm_source=chatgpt.com), [aistatement.com](https://aistatement.com/?utm_source=chatgpt.com)) Overall, the premise about collective action and shared risk is well\u2011supported, but the conclusion that governments and the public must impose and maintain a global moratorium is normative and contested.",
    "sources": []
  }
]