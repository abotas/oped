[
  {
    "claim_i_idx": 0,
    "claim_j_idx": 1,
    "delta_prob": 0.4,
    "reasoning": "If AI remains a complement and humans focus on distinctively human activities, it suggests meaningful aspects of human cognition aren\u2019t replicable by AI."
  },
  {
    "claim_i_idx": 0,
    "claim_j_idx": 2,
    "delta_prob": 0.3,
    "reasoning": "Complementarity implies AI is a different, narrower intelligence, though it doesn\u2019t guarantee the specific deficits listed."
  },
  {
    "claim_i_idx": 0,
    "claim_j_idx": 3,
    "delta_prob": 0.7,
    "reasoning": "Seeing AI as a tool that frees people strongly supports near-term productivity gains and democratized access."
  },
  {
    "claim_i_idx": 0,
    "claim_j_idx": 4,
    "delta_prob": 0.8,
    "reasoning": "A\u2019s vision directly aligns with net-positive use that highlights human distinctiveness and shifts time toward human-centered activities."
  },
  {
    "claim_i_idx": 0,
    "claim_j_idx": 5,
    "delta_prob": -0.9,
    "reasoning": "A future where AI complements humans conflicts with near-term human extinction from AI."
  },
  {
    "claim_i_idx": 0,
    "claim_j_idx": 6,
    "delta_prob": -0.6,
    "reasoning": "If AI is a beneficial complement, a sweeping indefinite global shutdown appears less warranted."
  },
  {
    "claim_i_idx": 0,
    "claim_j_idx": 7,
    "delta_prob": -0.6,
    "reasoning": "A suggests continued development can be justified and beneficial, undermining claims that progress is unjustifiable."
  },
  {
    "claim_i_idx": 0,
    "claim_j_idx": 8,
    "delta_prob": -0.3,
    "reasoning": "A reduces the salience of existential lethality framing and \u2018dangerous ignorance,\u2019 though it doesn\u2019t directly address consciousness."
  },
  {
    "claim_i_idx": 0,
    "claim_j_idx": 9,
    "delta_prob": -0.5,
    "reasoning": "The complementary, non-extinction view weakens the case for a global moratorium driven by survival-risk and collective action failure."
  },
  {
    "claim_i_idx": 1,
    "claim_j_idx": 0,
    "delta_prob": 0.5,
    "reasoning": "If AI cannot replicate human cognition and lacks consciousness/agency, it is more likely to complement rather than replace humans, emphasizing distinctively human work."
  },
  {
    "claim_i_idx": 1,
    "claim_j_idx": 3,
    "delta_prob": 0.6,
    "reasoning": "A\u2019s assertion that AI lacks embodied, unconscious, and value-laden cognition supports the view that AI is a narrower pattern-synthesizing intelligence that struggles with causality and narrative/temporal understanding."
  },
  {
    "claim_i_idx": 1,
    "claim_j_idx": 4,
    "delta_prob": 0.3,
    "reasoning": "Seeing AI as non-replicative of human minds makes its near-term role as a productivity aid and tutor more plausible."
  },
  {
    "claim_i_idx": 1,
    "claim_j_idx": 5,
    "delta_prob": 0.2,
    "reasoning": "If AI highlights human distinctiveness by lacking human-like consciousness and values, it modestly supports net-positive uses that accentuate uniquely human pursuits."
  },
  {
    "claim_i_idx": 1,
    "claim_j_idx": 6,
    "delta_prob": -0.5,
    "reasoning": "If AI lacks agency, consciousness, and cannot replicate human cognition, claims of near-term extinction from superhuman AI are significantly less plausible."
  },
  {
    "claim_i_idx": 1,
    "claim_j_idx": 7,
    "delta_prob": -0.4,
    "reasoning": "An indefinite, force-backed global shutdown is less justified if AI cannot replicate human minds and lacks agency, reducing perceived existential-risk urgency."
  },
  {
    "claim_i_idx": 1,
    "claim_j_idx": 8,
    "delta_prob": -0.3,
    "reasoning": "If AI is fundamentally non-human and non-agentic, the 'first-try terminal failure' rationale weakens, even if capabilities currently outpace alignment research."
  },
  {
    "claim_i_idx": 1,
    "claim_j_idx": 9,
    "delta_prob": -0.1,
    "reasoning": "Claim A supports that current AI lacks consciousness, undercutting the 'possibly self-aware soon' part, while being neutral on whether danger depends on consciousness."
  },
  {
    "claim_i_idx": 1,
    "claim_j_idx": 10,
    "delta_prob": -0.2,
    "reasoning": "If extinction risk is lower because AI cannot replicate human agency/consciousness, sweeping government-imposed moratoria are less warranted, though coordination problems may remain."
  },
  {
    "claim_i_idx": 2,
    "claim_j_idx": 0,
    "delta_prob": 0.5,
    "reasoning": "If AI is narrow and lacks causal/narrative/temporal understanding and embodiment, it is more likely to complement rather than replace distinctively human work."
  },
  {
    "claim_i_idx": 2,
    "claim_j_idx": 1,
    "delta_prob": 0.4,
    "reasoning": "A narrower, language-bound AI lacking embodied/implicit knowledge supports the view that human cognition\u2014integrating unconscious, emotional, and embodied aspects\u2014is not replicated by current AI."
  },
  {
    "claim_i_idx": 2,
    "claim_j_idx": 4,
    "delta_prob": 0.6,
    "reasoning": "Strength in pattern synthesis and language aligns with near-term boosts in tutoring, drafting, and democratized assistance, especially for less-skilled users."
  },
  {
    "claim_i_idx": 2,
    "claim_j_idx": 5,
    "delta_prob": 0.3,
    "reasoning": "A narrow AI that highlights human distinctiveness and aids learning/innovation modestly supports net-positive, human-centered outcomes, though societal uptake/misuse is outside the claim."
  },
  {
    "claim_i_idx": 2,
    "claim_j_idx": 6,
    "delta_prob": -0.3,
    "reasoning": "If AI is narrow and struggles with deeper reasoning, near-term superhuman extinction risk is less implied, reducing the likelihood of catastrophic outcomes from deployment."
  },
  {
    "claim_i_idx": 2,
    "claim_j_idx": 7,
    "delta_prob": -0.5,
    "reasoning": "Given AI\u2019s current limitations and complementary role, the case for an indefinite, worldwide shutdown of large training runs is weakened."
  },
  {
    "claim_i_idx": 2,
    "claim_j_idx": 8,
    "delta_prob": -0.2,
    "reasoning": "Narrow, language-bound capabilities make terminal-risk-from-first-try arguments less immediate, slightly undercutting the claim that continued development is unjustifiable."
  },
  {
    "claim_i_idx": 2,
    "claim_j_idx": 9,
    "delta_prob": -0.1,
    "reasoning": "The claim that scaling risks self-aware entities is weakened by AI\u2019s language-bound, non-embodied nature; the point that danger needn\u2019t rely on consciousness is largely unaffected."
  },
  {
    "claim_i_idx": 2,
    "claim_j_idx": 10,
    "delta_prob": -0.4,
    "reasoning": "If AI is limited and complementary, the necessity for a sweeping, enforced global moratorium framed around extinction risk is less compelling."
  },
  {
    "claim_i_idx": 3,
    "claim_j_idx": 0,
    "delta_prob": 0.5,
    "reasoning": "A shows AI as a productivity-boosting assistant helping humans, supporting complementarity (at least near term)."
  },
  {
    "claim_i_idx": 3,
    "claim_j_idx": 1,
    "delta_prob": 0.0,
    "reasoning": "A concerns practical impact, not whether minds are computational or conscious; no inference."
  },
  {
    "claim_i_idx": 3,
    "claim_j_idx": 2,
    "delta_prob": 0.2,
    "reasoning": "A\u2019s gains are in pattern-heavy tasks (tutoring, drafting), consistent with narrower pattern-synthesis strengths."
  },
  {
    "claim_i_idx": 3,
    "claim_j_idx": 5,
    "delta_prob": 0.5,
    "reasoning": "Democratized access and learning benefits align with net-positive use by most people and emphasize human roles."
  },
  {
    "claim_i_idx": 3,
    "claim_j_idx": 6,
    "delta_prob": -0.2,
    "reasoning": "Positive near-term outcomes weakly counter the claim that development is most likely to cause near-term extinction."
  },
  {
    "claim_i_idx": 3,
    "claim_j_idx": 7,
    "delta_prob": -0.3,
    "reasoning": "If AI broadly helps society now, an indefinite worldwide shutdown appears less warranted."
  },
  {
    "claim_i_idx": 3,
    "claim_j_idx": 8,
    "delta_prob": -0.2,
    "reasoning": "Demonstrated benefits make a blanket halt as \u201cunjustifiable to continue\u201d less compelling, though safety gaps may remain."
  },
  {
    "claim_i_idx": 3,
    "claim_j_idx": 9,
    "delta_prob": -0.1,
    "reasoning": "A doesn\u2019t address optimizer lethality; near-term helpfulness only slightly weakens the alarm about scaling."
  },
  {
    "claim_i_idx": 3,
    "claim_j_idx": 10,
    "delta_prob": -0.3,
    "reasoning": "Widespread benefits reduce the case for a global moratorium, even if industry coordination problems persist."
  },
  {
    "claim_i_idx": 4,
    "claim_j_idx": 0,
    "delta_prob": 0.8,
    "reasoning": "A says AI\u2019s net effect is to highlight human distinctiveness and free people for caregiving, teamwork, exploration, etc., which supports AI as a complementary tool rather than a replacement."
  },
  {
    "claim_i_idx": 4,
    "claim_j_idx": 1,
    "delta_prob": 0.3,
    "reasoning": "A\u2019s emphasis on human distinctiveness nudges toward the view that AI cannot replicate the full human mind, though it doesn\u2019t assert a strict philosophical non-equivalence."
  },
  {
    "claim_i_idx": 4,
    "claim_j_idx": 2,
    "delta_prob": 0.2,
    "reasoning": "A implies AI differs from embodied human understanding, modestly supporting the idea that AI is a narrower intelligence with limits relative to human cognition."
  },
  {
    "claim_i_idx": 4,
    "claim_j_idx": 3,
    "delta_prob": 0.8,
    "reasoning": "A explicitly claims most people use AI for learning/innovation with net positive outcomes (e.g., advances), aligning strongly with near-term productivity gains and democratized expertise."
  },
  {
    "claim_i_idx": 4,
    "claim_j_idx": 6,
    "delta_prob": -0.8,
    "reasoning": "A envisions sustained net positives and long-run human-centered benefits, which conflicts with near-term human extinction being the most likely outcome."
  },
  {
    "claim_i_idx": 4,
    "claim_j_idx": 7,
    "delta_prob": -0.6,
    "reasoning": "If A is true and AI\u2019s net impact is positive, an indefinite, force-backed global shutdown is less justified."
  },
  {
    "claim_i_idx": 4,
    "claim_j_idx": 8,
    "delta_prob": -0.7,
    "reasoning": "A implies benefits from continued development and use, contradicting the claim that proceeding is unjustifiable due to insurmountable safety gaps."
  },
  {
    "claim_i_idx": 4,
    "claim_j_idx": 9,
    "delta_prob": -0.4,
    "reasoning": "A\u2019s optimistic, net-positive trajectory lowers the likelihood that scaling poses existential danger irrespective of consciousness, though it doesn\u2019t directly address the consciousness point."
  },
  {
    "claim_i_idx": 4,
    "claim_j_idx": 10,
    "delta_prob": -0.6,
    "reasoning": "A undermines the premise for a global moratorium framed as a survival imperative, reducing the need for government-imposed shutdowns."
  },
  {
    "claim_i_idx": 5,
    "claim_j_idx": 0,
    "delta_prob": -0.8,
    "reasoning": "If superhuman AI is most likely to cause near-term extinction, a future where AI mainly complements humans becomes very unlikely."
  },
  {
    "claim_i_idx": 5,
    "claim_j_idx": 1,
    "delta_prob": 0.0,
    "reasoning": "Whether human cognition is equivalent to computation is orthogonal to AI posing extinction risk; A does not bear on consciousness or replication claims."
  },
  {
    "claim_i_idx": 5,
    "claim_j_idx": 2,
    "delta_prob": -0.5,
    "reasoning": "If superhuman, world-ending AI is feasible soon, the view that AI is inherently narrow and struggles with causality/narrative becomes less likely."
  },
  {
    "claim_i_idx": 5,
    "claim_j_idx": 3,
    "delta_prob": 0.0,
    "reasoning": "Near-term productivity gains can coexist with severe existential risk; A does not directly affect the likelihood of short-run boosts."
  },
  {
    "claim_i_idx": 5,
    "claim_j_idx": 4,
    "delta_prob": -0.9,
    "reasoning": "If AI development is most likely to end in near-term extinction, claims of net positive ultimate outcomes and highlighted human distinctiveness are almost certainly false."
  },
  {
    "claim_i_idx": 5,
    "claim_j_idx": 7,
    "delta_prob": 0.8,
    "reasoning": "If continued development likely causes extinction, a strict, indefinite global shutdown (even by force) becomes strongly supported."
  },
  {
    "claim_i_idx": 5,
    "claim_j_idx": 8,
    "delta_prob": 0.9,
    "reasoning": "A asserts we lack control and would fail on deployment, aligning with claims that capabilities outpace alignment and continued development is unjustifiable."
  },
  {
    "claim_i_idx": 5,
    "claim_j_idx": 9,
    "delta_prob": 0.8,
    "reasoning": "A supports that deadly risk doesn\u2019t depend on consciousness and that scaling further while ignorant is dangerously reckless."
  },
  {
    "claim_i_idx": 5,
    "claim_j_idx": 10,
    "delta_prob": 0.7,
    "reasoning": "If extinction is likely without intervention, relying on industry self-restraint is implausible and strong government-led moratoria become more likely."
  },
  {
    "claim_i_idx": 6,
    "claim_j_idx": 0,
    "delta_prob": -0.5,
    "reasoning": "If an indefinite global shutdown is necessary, forecasts that AI will seamlessly complement human work are less likely to materialize."
  },
  {
    "claim_i_idx": 6,
    "claim_j_idx": 1,
    "delta_prob": 0.0,
    "reasoning": "The shutdown rationale does not depend on whether human cognition equals computation; it can be motivated by risk regardless."
  },
  {
    "claim_i_idx": 6,
    "claim_j_idx": 2,
    "delta_prob": 0.0,
    "reasoning": "Claim A does not address AI\u2019s specific cognitive profile (causality, narrative, embodiment), so it neither supports nor opposes this."
  },
  {
    "claim_i_idx": 6,
    "claim_j_idx": 3,
    "delta_prob": -0.3,
    "reasoning": "A broad moratorium would constrain deployment and progress, making widespread near-term productivity gains less likely, despite existing evidence."
  },
  {
    "claim_i_idx": 6,
    "claim_j_idx": 4,
    "delta_prob": -0.5,
    "reasoning": "If AI warrants an indefinite shutdown, optimistic net-positive societal outcomes are less plausible under such constraints."
  },
  {
    "claim_i_idx": 6,
    "claim_j_idx": 5,
    "delta_prob": 0.9,
    "reasoning": "Only extinction-level or comparable catastrophic risk would justify an indefinite, enforced global halt; this strongly supports the claim."
  },
  {
    "claim_i_idx": 6,
    "claim_j_idx": 8,
    "delta_prob": 0.9,
    "reasoning": "An indefinite stop implies capabilities are outpacing alignment and that no workable safety plan exists, making continued development unjustifiable."
  },
  {
    "claim_i_idx": 6,
    "claim_j_idx": 9,
    "delta_prob": 0.8,
    "reasoning": "The call to halt regardless of consciousness aligns with the view that opaque, powerful optimizers are dangerous and scaling (e.g., to GPT-5) is reckless."
  },
  {
    "claim_i_idx": 6,
    "claim_j_idx": 10,
    "delta_prob": 1.0,
    "reasoning": "Claim A explicitly requires government-led, globally enforced moratoria, indicating industry cannot self-regulate; this is essentially the same thesis."
  },
  {
    "claim_i_idx": 7,
    "claim_j_idx": 0,
    "delta_prob": -0.5,
    "reasoning": "If superhuman AI is unjustifiable and failure could be terminal, the optimistic view that AI will merely complement humans (not replace or threaten them) is significantly less likely."
  },
  {
    "claim_i_idx": 7,
    "claim_j_idx": 1,
    "delta_prob": -0.3,
    "reasoning": "A implies computational systems can surpass humans without alignment; this undermines the claim that AI cannot replicate human cognition, though it doesn\u2019t require consciousness equivalence."
  },
  {
    "claim_i_idx": 7,
    "claim_j_idx": 2,
    "delta_prob": -0.4,
    "reasoning": "If capabilities are racing ahead and nearing superhuman levels, the view that AI is narrowly pattern-based and weak at causality/narrative is less likely to hold as a safety-relevant limitation."
  },
  {
    "claim_i_idx": 7,
    "claim_j_idx": 3,
    "delta_prob": 0.4,
    "reasoning": "Rapid capability gains make near-term productivity boosts more plausible, even if long-term continuation is unjustifiable."
  },
  {
    "claim_i_idx": 7,
    "claim_j_idx": 4,
    "delta_prob": -0.6,
    "reasoning": "Terminal-risk and lack of a safety plan conflict with the expectation of net-positive outcomes and a human-flourishing shift."
  },
  {
    "claim_i_idx": 7,
    "claim_j_idx": 5,
    "delta_prob": 0.8,
    "reasoning": "No viable safety plan and terminal first-failure imply that deploying superhuman AI under current conditions likely leads to human extinction."
  },
  {
    "claim_i_idx": 7,
    "claim_j_idx": 6,
    "delta_prob": 0.8,
    "reasoning": "If continuing development is unjustifiable and failure is terminal, a strong, indefinite global shutdown with robust enforcement becomes significantly more warranted."
  },
  {
    "claim_i_idx": 7,
    "claim_j_idx": 9,
    "delta_prob": 0.9,
    "reasoning": "Danger independent of consciousness and risks from scaling are reinforced by the facts that alignment lags, internals aren\u2019t well-understood, and failure would be terminal."
  },
  {
    "claim_i_idx": 7,
    "claim_j_idx": 10,
    "delta_prob": 0.7,
    "reasoning": "Given unjustifiability and industry incentives to continue, collective action and government intervention to enforce a moratorium becomes much more likely."
  },
  {
    "claim_i_idx": 8,
    "claim_j_idx": 0,
    "delta_prob": -0.5,
    "reasoning": "If powerful optimizing systems are lethal regardless of consciousness and we are scaling ignorantly, a benign complementary-only future becomes significantly less likely."
  },
  {
    "claim_i_idx": 8,
    "claim_j_idx": 1,
    "delta_prob": -0.1,
    "reasoning": "Claim A decouples risk from consciousness and allows that scaling could yield possibly self-aware entities, which slightly undercuts the assertion that AI lacks consciousness and cannot replicate aspects of human cognition; it does not address computational equivalence directly."
  },
  {
    "claim_i_idx": 8,
    "claim_j_idx": 2,
    "delta_prob": 0.0,
    "reasoning": "Claim A speaks to existential risk and interpretability, not to whether AI is narrowly pattern-based or struggles with causality and embodiment."
  },
  {
    "claim_i_idx": 8,
    "claim_j_idx": 3,
    "delta_prob": 0.0,
    "reasoning": "Existential risk from scaling does not bear on near-term productivity gains or the cited study; both can be true simultaneously."
  },
  {
    "claim_i_idx": 8,
    "claim_j_idx": 4,
    "delta_prob": -0.4,
    "reasoning": "If scaling entails dangerous ignorance and lethal potential, net-positive outcomes and a shift toward celebrating human distinctiveness become less likely overall."
  },
  {
    "claim_i_idx": 8,
    "claim_j_idx": 5,
    "delta_prob": 0.6,
    "reasoning": "A affirms lethal potential of powerful optimizers and our lack of interpretability/control, which supports the claim that superhuman AI development is likely to be catastrophically dangerous, though it does not by itself prove near-term extinction is most likely."
  },
  {
    "claim_i_idx": 8,
    "claim_j_idx": 6,
    "delta_prob": 0.5,
    "reasoning": "If current scaling is dangerously ignorant and risky regardless of consciousness, a brief pause seems insufficient and a stronger moratorium becomes significantly more plausible (though enforcement methods are policy choices not entailed by A)."
  },
  {
    "claim_i_idx": 8,
    "claim_j_idx": 7,
    "delta_prob": 0.8,
    "reasoning": "A directly supports that capabilities outpace alignment/mechanistic understanding and that proceeding could be terminal on first failure, making continued development unjustifiable."
  },
  {
    "claim_i_idx": 8,
    "claim_j_idx": 10,
    "delta_prob": 0.4,
    "reasoning": "Given existential danger and industry ignorance, the need for public/government intervention and a global moratorium becomes more plausible, though A does not itself establish a collective action failure."
  },
  {
    "claim_i_idx": 9,
    "claim_j_idx": 0,
    "delta_prob": -0.4,
    "reasoning": "If an extinction-risk-driven, government-enforced moratorium is needed, AI is not merely a benign complement; replacement/disruption risk and danger loom larger."
  },
  {
    "claim_i_idx": 9,
    "claim_j_idx": 1,
    "delta_prob": 0.0,
    "reasoning": "Claim A is agnostic about computational equivalence or consciousness; the moratorium rationale does not depend on this view."
  },
  {
    "claim_i_idx": 9,
    "claim_j_idx": 2,
    "delta_prob": -0.3,
    "reasoning": "Needing a global moratorium for extinction risk conflicts with the idea that AI is simply narrow and limited to correlations."
  },
  {
    "claim_i_idx": 9,
    "claim_j_idx": 3,
    "delta_prob": 0.1,
    "reasoning": "A halt on frontier training does not contradict observed near-term productivity gains; both can hold simultaneously."
  },
  {
    "claim_i_idx": 9,
    "claim_j_idx": 4,
    "delta_prob": -0.6,
    "reasoning": "If coercive global intervention is required due to extinction risk, optimistic net-positive expectations are undermined."
  },
  {
    "claim_i_idx": 9,
    "claim_j_idx": 5,
    "delta_prob": 0.7,
    "reasoning": "Treating AI as a shared survival threat supports the view that developing superhuman AI under current conditions is likely catastrophic."
  },
  {
    "claim_i_idx": 9,
    "claim_j_idx": 6,
    "delta_prob": 0.7,
    "reasoning": "Claim A calls for a maintained global moratorium enforced by governments/public, aligning with an indefinite, enforceable shutdown (though not necessarily every enforcement detail)."
  },
  {
    "claim_i_idx": 9,
    "claim_j_idx": 7,
    "delta_prob": 0.8,
    "reasoning": "If industry cannot self-regulate and extinction-risk failure is terminal, continuing development is unjustifiable\u2014hence a moratorium."
  },
  {
    "claim_i_idx": 9,
    "claim_j_idx": 8,
    "delta_prob": 0.6,
    "reasoning": "Framing the risk as extinction-level supports danger from powerful optimizers regardless of consciousness and cautions against scaling amid poor interpretability."
  }
]