{
  "doc_id": "1_yoshua_b",
  "doc_title": "1. Yoshua Bengio: Debating AI Safety Risks",
  "claim_idx": 7,
  "claim": "Because aggregated expert judgments (e.g., a median ~5% extinction estimate) and rational decision theory imply non-negligible expected losses, dismissing AI x-risk as a form of Pascal\u2019s wager or as unknowable is unjustified \u2014 policy should follow the precautionary principle and invest urgently in safety research, regulation and multilateral oversight even amid deep uncertainty.",
  "veracity": 90,
  "explanation": "Substantially accurate. Large, recent expert-elicitation exercises do report a median \u22485% chance of \u2018extremely bad\u2019 outcomes (including human extinction) from advanced AI, and those results have been widely reported and documented. ([arxiv.org](https://arxiv.org/abs/2401.02843?utm_source=chatgpt.com), [vox.com](https://www.vox.com/future-perfect/2024/1/10/24032987/ai-impacts-survey-artificial-intelligence-chatgpt-openai-existential-risk-superintelligence?utm_source=chatgpt.com)) Decision-theoretic expected\u2011value reasoning implies that a non\u2011zero probability of extremely large (existential) loss produces a non\u2011negligible expected loss, which is the core of the Pascal\u2011style argument for precaution; this follows from standard expected\u2011utility formulations (though it invites well\u2011known decision\u2011theory objections). ([en.wikipedia.org](https://en.wikipedia.org/wiki/Expected_utility_hypothesis?utm_source=chatgpt.com)) At the same time, philosophers and decision theorists warn about paradoxes (e.g., \u201cPascal\u2019s mugging\u201d) and priors/weighting choices that complicate naive EV calculations \u2014 so implementation requires careful formalization rather than blind application. ([en.wikipedia.org](https://en.wikipedia.org/wiki/Pascal%27s_mugging?utm_source=chatgpt.com), [cambridge.org](https://www.cambridge.org/core/journals/utilitas/article/abs/pascals-mugger-strikes-again/6BEFD5AF0C3C2E7DAE8F7EE1F47FDD2E?utm_source=chatgpt.com)) Finally, leading scholars and many governments have argued for and begun to implement precautionary measures (funding safety research, testing regimes, and multilateral initiatives such as the UK\u2019s 2023 AI Safety Summit / Bletchley Declaration), supporting the recommendation that policy act urgently under deep uncertainty. ([tobyord.com](https://www.tobyord.com/writing/the-precipice-revisited?utm_source=chatgpt.com), [gov.uk](https://www.gov.uk/government/publications/ai-safety-summit-2023-the-bletchley-declaration?utm_source=chatgpt.com)) Overall, the claim fairly represents current expert judgment, the import of expected\u2011value reasoning, and mainstream policy prescriptions \u2014 while also overlooking important technical caveats about how to treat tiny probabilities and how to operationalize precaution in practice.",
  "sources": []
}