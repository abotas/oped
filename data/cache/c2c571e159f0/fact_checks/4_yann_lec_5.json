{
  "doc_id": "4_yann_lec",
  "doc_title": "4. Yann LeCun on Open Source AI and AGI",
  "claim_idx": 5,
  "claim": "AGI/superintelligence is unlikely to arrive as a sudden, single catastrophic event; progress will be gradual and iterative (more like engineering design than a one-off discovery), so doomer scenarios that assume a rapid rogue AI takeover rest on false assumptions (e.g., innate agentic drive to dominate) and neglect that safer, controllable systems and countermeasures will co-develop and be widely deployed.",
  "veracity": 70,
  "explanation": "Partly true. Many leading actors treat AGI as more likely to emerge through incremental, engineering-style advances and advocate iterative deployment and layered defenses (e.g., OpenAI\u2019s \u2018\u2018continuous/iterative\u2019\u2019 safety approach). ([openai.com](https://openai.com/safety/how-we-think-about-safety-alignment/?utm_source=chatgpt.com)) However, prominent theorists also describe plausible \u2018\u2018fast/hard takeoff\u2019\u2019 scenarios and instrumental convergent drives that could enable rapid self\u2011improvement and takeover in some designs, so catastrophic outcomes can\u2019t be dismissed out of hand. ([en.wikipedia.org](https://en.wikipedia.org/wiki/Superintelligence%3A_Paths%2C_Dangers%2C_Strategies?utm_source=chatgpt.com), [selfawaresystems.com](https://selfawaresystems.com/2007/11/30/paper-on-the-basic-ai-drives/?utm_source=chatgpt.com)) Empirical work shows current LLMs sometimes behave agentically under specific prompts but do not yet demonstrate spontaneous, robust goal\u2011directed self\u2011preservation\u2014i.e., agentic drive is not simply guaranteed. ([anthropic.com](https://www.anthropic.com/research/agentic-misalignment?utm_source=chatgpt.com)) Expert surveys show wide disagreement about timelines, so uncertainty about speed of transition remains material. ([wiki.aiimpacts.org](https://wiki.aiimpacts.org/ai_timelines/predictions_of_human-level_ai_timelines/ai_timeline_surveys/2023_expert_survey_on_progress_in_ai?utm_source=chatgpt.com)) Finally, governments and industry are building safety institutes, regulations and technical mitigations (UK AI Safety Institute, EU AI Act, various industry safety frameworks), but their global coverage and effectiveness before any disruptive transition are not guaranteed. ([gov.uk](https://www.gov.uk/government/publications/ai-safety-institute-overview/introducing-the-ai-safety-institute?utm_source=chatgpt.com), [digital-strategy.ec.europa.eu](https://digital-strategy.ec.europa.eu/en/policies/regulatory-framework-ai?utm_source=chatgpt.com))",
  "sources": []
}