{
  "doc_id": "1_yoshua_b",
  "doc_title": "1. Yoshua Bengio: Debating AI Safety Risks",
  "claim_idx": 0,
  "claim": "Humanity is racing toward AGI (human-level AI) and ASI (superhuman AI) while there is currently no reliable technical method or scientific assurance to make such systems behave morally or as intended, so the possibility they could pursue incompatible goals (including harming or eliminating humans) requires urgent attention and precaution.",
  "veracity": 90,
  "explanation": "Largely accurate. Multiple large expert surveys and recent progress in foundation models give many researchers a substantial probability that human-level AI (AGI) could arrive within decades, and industry competition makes this a rapid, contested race. ([arxiv.org](https://arxiv.org/abs/2401.02843?utm_source=chatgpt.com)) Major researchers and organisations (e.g., Future of Life Institute, Center for AI Safety) have publicly warned of catastrophic/existential risks and called for urgent precaution. ([futureoflife.org](https://futureoflife.org/open-letter/pause-giant-ai-experiments/?utm_source=chatgpt.com), [theverge.com](https://www.theverge.com/2023/5/30/23742005/ai-risk-warning-22-word-statement-google-deepmind-openai?utm_source=chatgpt.com)) At the same time, leading labs (e.g., OpenAI) and safety experts acknowledge that alignment/control remains an open, unsolved research problem and that current techniques (RLHF, policy layers, red\u2011teaming, etc.) do not provide scientific assurance that superhuman systems will behave morally or as intended. ([openai.com](https://openai.com/safety/how-we-think-about-safety-alignment/?utm_source=chatgpt.com), [time.com](https://time.com/6958868/artificial-intelligence-safety-evaluations-risks/?utm_source=chatgpt.com)) Philosophers and theorists have also articulated how mis-specified goals can lead powerful agents to pursue harmful instrumental subgoals (the \u2018control\u2019/instrumental\u2011convergence concern). ([en.wikipedia.org](https://en.wikipedia.org/wiki/Superintelligence%3A_Paths%2C_Dangers%2C_Strategies?utm_source=chatgpt.com)) Because of (a) non\u2011negligible expert probability estimates, (b) clear competitive incentives, and (c) the absence of proven, scalable alignment guarantees, urgent attention and precaution are justified\u2014though exact timelines and probabilities remain uncertain, which is why the score is high but not 100.",
  "sources": []
}