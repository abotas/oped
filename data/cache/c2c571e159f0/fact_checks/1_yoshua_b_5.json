{
  "doc_id": "1_yoshua_b",
  "doc_title": "1. Yoshua Bengio: Debating AI Safety Risks",
  "claim_idx": 5,
  "claim": "Open-sourcing AGI code and weights is not an unambiguous safety strategy \u2014 while it aids research, it makes attacks and fine-tuning-based misuse easier, is harder to patch once released, and decisions about openness should be made democratically with safeguards rather than left to CEOs or unregulated markets.",
  "veracity": 92,
  "explanation": "Largely accurate. Open-sourcing weights/code does help research, transparency and reuse but also creates concrete new risks: open models have enabled criminal \u2018dark\u2019 LLMs and abuse (e.g., WormGPT/FraudGPT). ([news.sophos.com](https://news.sophos.com/en-us/2023/11/28/cybercriminals-cant-agree-on-gpts/?utm_source=chatgpt.com)) Academic work documents security/privacy trade\u2011offs of open releases and recommends safety frameworks for open models. ([arxiv.org](https://arxiv.org/abs/2406.10415?utm_source=chatgpt.com)) Recent research shows fine\u2011tuning or downstream chains can be abused to exfiltrate data or introduce backdoors, making misuse easier. ([arxiv.org](https://arxiv.org/abs/2505.15656?utm_source=chatgpt.com)) Companies and safety teams note that open\u2011weight releases reduce the effectiveness of system\u2011level safeguards and are harder to remediate once widely distributed. ([openai.com](https://openai.com/index/updating-our-preparedness-framework/?utm_source=chatgpt.com)) Because of these trade\u2011offs, many experts and governments argue release decisions need public oversight, governance and safeguards rather than purely market/CEO choices (UK establishing an AI Safety Institute and parliamentary scrutiny). ([gov.uk](https://www.gov.uk/government/publications/ai-safety-institute-overview/introducing-the-ai-safety-institute?utm_source=chatgpt.com)) Overall the claim is well\u2011supported, though context matters (open models also enable beneficial research), so it is not simply black\u2011and\u2011white.",
  "sources": []
}