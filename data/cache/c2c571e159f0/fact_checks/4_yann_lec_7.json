{
  "doc_id": "4_yann_lec",
  "doc_title": "4. Yann LeCun on Open Source AI and AGI",
  "claim_idx": 7,
  "claim": "AI safety will be an iterative engineering problem rather than a single provable fix: practical guardrails should be integrated into objective-driven architectures (e.g., include constraints like \u2018obey humans\u2019 or \u2018do no harm\u2019 in the energy/objective function), and safety will be improved progressively through testing and refinement\u2014analogous to how complex, high-stakes systems (like jet engines) became reliable over decades.",
  "veracity": 80,
  "explanation": "Mostly accurate (score 80). Many leading researchers and practitioners treat AI safety as an ongoing, empirical engineering problem\u2014emphasizing iterative methods like reward modeling, scalable oversight/iterated amplification, and red\u2011teaming rather than expecting a single, universally provable fix. Concrete research lists and agendas (e.g., DeepMind\u2019s \u201cConcrete Problems in AI Safety\u201d and Paul Christiano\u2019s scalable-oversight ideas) explicitly frame safety as a set of practical, iterated engineering challenges.([arxiv.org](https://arxiv.org/abs/1606.06565?utm_source=chatgpt.com), [effectivealtruism.com](https://www.effectivealtruism.com/articles/paul-christiano-current-work-in-ai-alignment?utm_source=chatgpt.com)) Formal verification work (e.g., Reluplex) shows proofs are possible in restricted cases but are not yet general solutions for large, goal-directed models, so provable fixes are limited in scope today.([arxiv.org](https://arxiv.org/abs/1702.01135v1?utm_source=chatgpt.com)) Industry practice also relies on iterative testing and red\u2011teaming to discover and patch failures.([arxiv.org](https://arxiv.org/abs/2503.16431?utm_source=chatgpt.com)) Integrating \u201cobey humans\u201d or \u201cdo no harm\u201d constraints into objectives is a common proposal, but it repeatedly runs into specification\u2011gaming/reward\u2011hacking problems\u2014so constraints help but aren\u2019t a panacea; that argues for progressive testing, monitoring, and hybrid approaches (empirical mitigations + formal methods + governance).([arxiv.org](https://arxiv.org/abs/1606.06565?utm_source=chatgpt.com), [en.wikipedia.org](https://en.wikipedia.org/wiki/Reward_hacking?utm_source=chatgpt.com)) The jet\u2011engine analogy is useful at a high level because complex engineered systems did improve reliability over decades, but AI\u2019s goal-directed, self\u2011improving nature and specification\u2011gaming risks make the analogy imperfect \u2014 iterative engineering is necessary and likely the dominant near\u2011term path, but not guaranteed to be sufficient on its own.([wired.com](https://www.wired.com/2016/07/rare-look-inside-vault-rebuilt-jet-engines-prove-can-fly?utm_source=chatgpt.com))",
  "sources": []
}