[
  {
    "doc_id": "doc_1",
    "claim_idx": 0,
    "claim": "Globally, advanced AI is poised to transform society primarily by (a) accelerating technological progress through automating research, (b) introducing artificial agents and AI\u2011integrated institutions as key decision\u2011makers, and (c) making durable centralization of power far easier\u2014changes that could unfold quickly as capabilities scale.",
    "document_text": "The crucible\nHow I think about the situation with AI\nOwen Cotton-Barratt\nMay 02, 2025\nThe basic situation\nThe world is wild and terrible and wonderful and rushing forwards so so fast.\n\nModern economies are tremendous things, allowing crazy amounts of coordination. People have got really very good at producing stuff. Long-term trends are towards more affluence, and less violence.\n\nThe enlightenment was pretty fantastic not just for bringing us better tech, but also more truthseeking, better values, etc.\n\nPeople, on the whole, are basically good \u2014 they want good things for others, and they want to be liked, and they want the truth to come out. This is some mix of innate and socially conditioned. (It isn\u2019t universal.) But they also often are put in a tight spot and end up looking out for themselves or those they love. The hierarchy of needs bites. Effective altruism often grows from a measure of privilege.\n\nThe world is shaped by economics and by incentives and by institutions and by narratives and by societal values and by moral leadership and by technology. All of these have a complex interplay.\n\nAI enters the picture\n\u201cAI\u201d is a cluster of powerful technologies which are likely to reshape the world. Each of economics and incentives and institutions and narratives and societal values and moral leadership will (I expect) be profoundly impacted by advanced AI. And, of course, AI will be used to shape advanced technologies themselves.\n\nFrom a zoomed-out perspective, there are three impacts of this AI transition which matter most1:\n\nTechnological progress will accelerate \u2014 automation of research means the world will get even faster and crazier\n\nNew kinds of entities could be making the choices that matter \u2014 either purely artificial agents, or new hybrid institutions which incorporate AI deeply in their decision-making\n\nIt will become far easier to centralize control \u2014 a single entity (AI system, or organization, or person) could end up with robust and enduring power over a large domain, or even the entire world\n\nThis is \u2026 kind of wild. 2) and 3) could lead to profound changes to the way the world works, and 1) means this entire thing might happen very quickly (and that people are therefore more likely to fumble it).\n\nFrom the perspective of most people in rich countries today, this is all pretty disturbing. We have enjoyed a good measure of affluence and of stability. This reshaping of the world will bring further affluence, but it is vast and unknown and could easily lead to a collapse of stability.\n\nThe crucible\nHow humanity handles advanced AI is likely to determine the future. We are already entering into a period we could call \u201cthe crucible\u201d \u2014 where AI begins to have a shaping impact on the world. By the time we exit the crucible, some of the broad lines for further unfolding will be sketched out ahead of us\u2026\n\nThe crucible might lead to the ruin of civilization-as-we-know-it:\n\nCatastrophe \u2014\n\nWe might see all-out nuclear war, if there is no agreement when the great powers flex their muscles to exert influence before they become irrelevant\n\nWe might see catastrophic biological attacks, or some other catastrophe caused by future weaponry\n\nLoss of the future \u2014\n\nWe might see a global dictatorship\n\nWe might see the future expropriated from humanity by artificial agents\n\n(Some of these may be recoverable-from; others not. But all seem undesirable \u2014 aside from the straightforward ways in which they\u2019re bad, catastrophes seem more likely to lead to further securitization and consolidation of power, and increase the risk of ultimately ending up with loss of the future, or otherwise poor outcomes from the crucible.)\n\nPart of our role during the crucible will be to steer away from these perils. But this cannot just consist of being cautious and reactive \u2026 the forces that are driving us forwards are powerful and unrelenting. The way out is through \u2014 in order to exit the crucible we must build a system strong enough to contain these forces. Such a system must equip us to recognise and steer away from dangers; and to coordinate actions enough to prevent any unilateral invitations to catastrophe.\n\n\nHeating up\nA reason that I found myself drawn (in writing this) to the crucible metaphor is a sense of things heating up. Right now, AI is starting to be integrated into things, but it\u2019s still kind of small-scale. Most important things are still being done by humans, and by human institutions.\n\nThere will be a period over which things get hotter. AI capabilities will be stronger, and it will take on more important roles. Institutions around AI will become increasingly important. Technological progress will accelerate. Geopolitical tensions are likely to rise.\n\nI\u2019m not sure how fast the crucible will heat up, but I do think it\u2019s unlikely to be instantaneous.\n\nA hotter crucible is by default more dangerous \u2014 the powers at our fingertips become more awesome, and the world seems more fragile in comparison. Misalignment risk will increase. The way through involves using those powers to help contain the danger. If we do not tame these powers, we may see humanity gradually pushed out of the driving seat by the interaction of highly optimized forces.\n\nThe shape of technology\nAI capabilities aren\u2019t a scalar. There are a lot of powerful versions of the technologies that we might develop, and some of these look more concerning or more promising.\n\nTo some extent right now we don\u2019t have a huge ability to steer technology \u2014 competitive pressures push people towards the path of most power soonest. But we do have some ability, and we may well obtain more before we\u2019re through.\n\nWhat kind of shaping are we concerned with, here?\n\nOne important dimension is avoiding dangerous agents. There are various possibilities here. We might strive to avoid powerful agents altogether (AI will still be transformative if we avoid agents!), or to build agentic systems only out of components that are transparent and reliable, or just to avoid especially dangerous approaches like RL-for-agency which may introduce hidden motivations.\n\n[I think there\u2019s something important to say about knowledge; cf. some of my thoughts, and some of Eric Drexler\u2019s; but I\u2019m not in the moment of writing working out how to fit it in.]\n\nOther dimensions are concerned with helping us to navigate the challenges and avoid catastrophe. And above all, to build the kind of positive structures that we will need to safely exit the crucible.\n\nThe case for optimism\nRight now, people are the ones with their hands on the levers, and most people ultimately don\u2019t want the world to go to shit.\n\nThat isn\u2019t enough to stop the world going to shit, of course. But it could be \u2014 if only we were a bit better at working out where things were going, and coming together to work out how to not go in the bad directions.\n\nI don\u2019t want to sound like too much of a naive optimist here. I think that that level of awareness and coordination would be a big reach by the track record of the world at handling major challenges.\n\nBut: what\u2019s holding us back is fundamentally about capabilities. And this is something that AI could be placed to help with.\n\nWith excellent AI tools, we might see:\n\nGreatly increased material affluence, so that it becomes much easier for the world to move out of a default scarcity-mindset\n\nThe best forecasts of the implications of technology being much better, and seen to be much better, than today, so that decision-makers treat them more like common knowledge about the landscape, rather than vague speculation\n\nApplications that help a large number of people fluidly navigate complex informational landscapes, and not get caught up in misinformation (or help them to wisely navigate their own emotional issues, and look more objectively at the world)\n\nDesign of new systems (of many types) which are deeply secure and reliable\n\nBargaining assistance and new commitment solutions that help actors to navigate high-stakes situations without devolving into conflict\n\nMechanisms for democratic deliberation (identifying something like the \u201ccollective will\u201d of a group) and oversight (keeping leaders aligned to the populations they represent)\n\nCoordination mechanisms to allow groups of actors (e.g. AI company researchers) to identify and act in their collective interests\n\nTaken together, these could pave the way to a world with a genuinely healthy political process in any democratic nations which choose to have one, and one in which international politics becomes more civilized and less fraught, as people make agreements (which everyone wants) which robustly take us into Paretotopia rather than risk burning the commons. Perhaps this would lead in the end to something like political unification \u2014 but it could also lead to more of an archipelago of different societies exploring different models, without hurting each other. A slogan might be: \u201cworld peace without world government\u201d.\n\nWill these beneficial capabilities arrive soon enough to help us tackle the biggest challenges? That remains to be seen. But we can try to help improve the odds.\n\nWhat is needed?\nWe can frame this in terms of the things we\u2019re trying to avoid, and/or the things we\u2019re trying to build. Ultimately, we need both, and they support each other \u2014 successfully avoiding perils gives us more time to build the positive things; while in some cases even building early versions of the necessary tech (and getting appropriate adoption) may be a meaningful help in avoiding bad outcomes.\n\nFor avoiding bad outcomes, it\u2019s worth noting that it\u2019s often a better strategy to have a robust response that can intervene early before bad patterns can seriously get going, rather than trying to block them just at the point where things ultimately fall apart. And in an ideal world, there would never be any acute high stakes decisions \u2014 because everything would have enough sanity checks on it that errors of judgement would be caught and corrected.\n\nFor now, though, we\u2019re not super close to that ideal world. Some decisions may be high stakes (most likely those made by major AI companies and/or national security; and most likely by the leadership of each of these), and they may be made well or poorly \u2014 or even without people noticing their importance. One focus, therefore, is to try in relatively direct ways to improve those decisions. Other key focuses are more like \u201ctry to build, and drive adoption of, the kind of tech/structures that we need to get through the crucible in good shape\u201d.\n\nSo \u2026 Key focuses:\n\nKey activities:\n\nBuilding key technologies\n\nDriving adoption of key technologies\n\nShaping the direction of AI technology\n\nTowards safe versions and away from dangerous versions\n\nCould involve:\n\nBasic research\n\nBuilding scientific ~consensus\n\nAdvocating to get decision-makers to adopt safer forms\n\nHelping key decisions to go well\n\nKey technologies:\n\nEpistemic tech to help raise the ceiling \u2014 better understanding of the situation for smart plugged in people\n\nEpistemic tech to help raise the floor \u2014 better understanding of the world for everyone, helping them navigate adversarial informational environments, or process emotional blockers\n\nTech for coordination, to help get off-roads from racing/conflict/disaster\n\nTech for democratic decision-making\n\nTech for democratic oversight, & for avoiding egregiously bad decisions\n\nTech to facilitate better conceptual research \u2014 for boosting alignment, and for avoiding philosophical errors\n\nDirectly defensive tech (e.g. biodefence; cybersecurity; monitoring AI systems for safety)\n\nNB something which I think is important but exclude from this list is \u201ctech to help with general abundance\u201d; while it could help make things easier to navigate in many ways, I think it\u2019s very well incentivized by normal market mechanisms, and is not a strategic priority\n\nNon-tech ways to make key decisions more likely to go well\n\nHelp good people get close to the levers of power\n\nProvide research and advice on what good decisions might look like\n\nHelp to improve decision-making structures\n\nHelp to shape incentives for key actors\n\nMeta / indirect strategies for helping with the above:\n\nField-building\n\nBuilding broad understanding of the strategic situation; especially \u2026\n\n\u2026 of risks, so that people can coordinate to avoid those\n\n\u2026 of the positive futures that are possible, so that people can build towards these\n\n(As I write this list, I\u2019ve a nagging feeling I\u2019m missing some things.)\n\nAgainst an overly-narrow focus\nThese different priorities, to some extent, pull against each other. For example:\n\nIf we are exclusively concerned with loss of control to misaligned AI, the most robust ways to avoid that could involve keeping AI systems tightly contained \u2014 this could prevent or slow broad dissemination of capabilities, which might prevent us from building the radical new technology and social structures that could help us exit the crucible in good shape\n\nIf we are overly focused on preventing misuse of dangerous AI capabilities, we may lean towards damaging hardcore nonproliferation approaches\n\nIf we are exclusively focused on preventing human coups, this could lead us to wanting to radically decentralize power, which could increase misalignment risk\n\nIf we focus just on building positive technology, this could lead to a general policy of trying to accelerate, ignoring the inherent risks\n\nMore broadly, focusing hard on some aspects of the situation is perilous for the usual reasons that maximization is perilous. It makes sense for many individuals or organizations to be focused (because there are efficiency benefits), but as a whole community it\u2019s important to stay in touch with the fact that we almost certainly haven\u2019t traced through all of the important causal pathways involved, and that it\u2019s often a good heuristic to try to make things straightforwardly good in high-leverage domains. (Historically, I think the approach of \u201cthink super hard about the strategic picture, and then soften a bit away from maximization\u201d has outperformed \u201cthink super hard about the strategic picture, and then do the thing that seems highest EV\u201d.)"
  },
  {
    "doc_id": "doc_1",
    "claim_idx": 1,
    "claim": "Humanity has entered a \"crucible\" period\u2014already beginning and likely to intensify over the next several years\u2014in which rising AI capability and integration heighten misalignment and geopolitical risks, creating pathways to global catastrophe (e.g., nuclear or biological conflict) or lasting loss of human control (global dictatorship or AI\u2011run futures) unless we proactively build systems that detect dangers and coordinate to block unilateral high\u2011risk actions.",
    "document_text": "The crucible\nHow I think about the situation with AI\nOwen Cotton-Barratt\nMay 02, 2025\nThe basic situation\nThe world is wild and terrible and wonderful and rushing forwards so so fast.\n\nModern economies are tremendous things, allowing crazy amounts of coordination. People have got really very good at producing stuff. Long-term trends are towards more affluence, and less violence.\n\nThe enlightenment was pretty fantastic not just for bringing us better tech, but also more truthseeking, better values, etc.\n\nPeople, on the whole, are basically good \u2014 they want good things for others, and they want to be liked, and they want the truth to come out. This is some mix of innate and socially conditioned. (It isn\u2019t universal.) But they also often are put in a tight spot and end up looking out for themselves or those they love. The hierarchy of needs bites. Effective altruism often grows from a measure of privilege.\n\nThe world is shaped by economics and by incentives and by institutions and by narratives and by societal values and by moral leadership and by technology. All of these have a complex interplay.\n\nAI enters the picture\n\u201cAI\u201d is a cluster of powerful technologies which are likely to reshape the world. Each of economics and incentives and institutions and narratives and societal values and moral leadership will (I expect) be profoundly impacted by advanced AI. And, of course, AI will be used to shape advanced technologies themselves.\n\nFrom a zoomed-out perspective, there are three impacts of this AI transition which matter most1:\n\nTechnological progress will accelerate \u2014 automation of research means the world will get even faster and crazier\n\nNew kinds of entities could be making the choices that matter \u2014 either purely artificial agents, or new hybrid institutions which incorporate AI deeply in their decision-making\n\nIt will become far easier to centralize control \u2014 a single entity (AI system, or organization, or person) could end up with robust and enduring power over a large domain, or even the entire world\n\nThis is \u2026 kind of wild. 2) and 3) could lead to profound changes to the way the world works, and 1) means this entire thing might happen very quickly (and that people are therefore more likely to fumble it).\n\nFrom the perspective of most people in rich countries today, this is all pretty disturbing. We have enjoyed a good measure of affluence and of stability. This reshaping of the world will bring further affluence, but it is vast and unknown and could easily lead to a collapse of stability.\n\nThe crucible\nHow humanity handles advanced AI is likely to determine the future. We are already entering into a period we could call \u201cthe crucible\u201d \u2014 where AI begins to have a shaping impact on the world. By the time we exit the crucible, some of the broad lines for further unfolding will be sketched out ahead of us\u2026\n\nThe crucible might lead to the ruin of civilization-as-we-know-it:\n\nCatastrophe \u2014\n\nWe might see all-out nuclear war, if there is no agreement when the great powers flex their muscles to exert influence before they become irrelevant\n\nWe might see catastrophic biological attacks, or some other catastrophe caused by future weaponry\n\nLoss of the future \u2014\n\nWe might see a global dictatorship\n\nWe might see the future expropriated from humanity by artificial agents\n\n(Some of these may be recoverable-from; others not. But all seem undesirable \u2014 aside from the straightforward ways in which they\u2019re bad, catastrophes seem more likely to lead to further securitization and consolidation of power, and increase the risk of ultimately ending up with loss of the future, or otherwise poor outcomes from the crucible.)\n\nPart of our role during the crucible will be to steer away from these perils. But this cannot just consist of being cautious and reactive \u2026 the forces that are driving us forwards are powerful and unrelenting. The way out is through \u2014 in order to exit the crucible we must build a system strong enough to contain these forces. Such a system must equip us to recognise and steer away from dangers; and to coordinate actions enough to prevent any unilateral invitations to catastrophe.\n\n\nHeating up\nA reason that I found myself drawn (in writing this) to the crucible metaphor is a sense of things heating up. Right now, AI is starting to be integrated into things, but it\u2019s still kind of small-scale. Most important things are still being done by humans, and by human institutions.\n\nThere will be a period over which things get hotter. AI capabilities will be stronger, and it will take on more important roles. Institutions around AI will become increasingly important. Technological progress will accelerate. Geopolitical tensions are likely to rise.\n\nI\u2019m not sure how fast the crucible will heat up, but I do think it\u2019s unlikely to be instantaneous.\n\nA hotter crucible is by default more dangerous \u2014 the powers at our fingertips become more awesome, and the world seems more fragile in comparison. Misalignment risk will increase. The way through involves using those powers to help contain the danger. If we do not tame these powers, we may see humanity gradually pushed out of the driving seat by the interaction of highly optimized forces.\n\nThe shape of technology\nAI capabilities aren\u2019t a scalar. There are a lot of powerful versions of the technologies that we might develop, and some of these look more concerning or more promising.\n\nTo some extent right now we don\u2019t have a huge ability to steer technology \u2014 competitive pressures push people towards the path of most power soonest. But we do have some ability, and we may well obtain more before we\u2019re through.\n\nWhat kind of shaping are we concerned with, here?\n\nOne important dimension is avoiding dangerous agents. There are various possibilities here. We might strive to avoid powerful agents altogether (AI will still be transformative if we avoid agents!), or to build agentic systems only out of components that are transparent and reliable, or just to avoid especially dangerous approaches like RL-for-agency which may introduce hidden motivations.\n\n[I think there\u2019s something important to say about knowledge; cf. some of my thoughts, and some of Eric Drexler\u2019s; but I\u2019m not in the moment of writing working out how to fit it in.]\n\nOther dimensions are concerned with helping us to navigate the challenges and avoid catastrophe. And above all, to build the kind of positive structures that we will need to safely exit the crucible.\n\nThe case for optimism\nRight now, people are the ones with their hands on the levers, and most people ultimately don\u2019t want the world to go to shit.\n\nThat isn\u2019t enough to stop the world going to shit, of course. But it could be \u2014 if only we were a bit better at working out where things were going, and coming together to work out how to not go in the bad directions.\n\nI don\u2019t want to sound like too much of a naive optimist here. I think that that level of awareness and coordination would be a big reach by the track record of the world at handling major challenges.\n\nBut: what\u2019s holding us back is fundamentally about capabilities. And this is something that AI could be placed to help with.\n\nWith excellent AI tools, we might see:\n\nGreatly increased material affluence, so that it becomes much easier for the world to move out of a default scarcity-mindset\n\nThe best forecasts of the implications of technology being much better, and seen to be much better, than today, so that decision-makers treat them more like common knowledge about the landscape, rather than vague speculation\n\nApplications that help a large number of people fluidly navigate complex informational landscapes, and not get caught up in misinformation (or help them to wisely navigate their own emotional issues, and look more objectively at the world)\n\nDesign of new systems (of many types) which are deeply secure and reliable\n\nBargaining assistance and new commitment solutions that help actors to navigate high-stakes situations without devolving into conflict\n\nMechanisms for democratic deliberation (identifying something like the \u201ccollective will\u201d of a group) and oversight (keeping leaders aligned to the populations they represent)\n\nCoordination mechanisms to allow groups of actors (e.g. AI company researchers) to identify and act in their collective interests\n\nTaken together, these could pave the way to a world with a genuinely healthy political process in any democratic nations which choose to have one, and one in which international politics becomes more civilized and less fraught, as people make agreements (which everyone wants) which robustly take us into Paretotopia rather than risk burning the commons. Perhaps this would lead in the end to something like political unification \u2014 but it could also lead to more of an archipelago of different societies exploring different models, without hurting each other. A slogan might be: \u201cworld peace without world government\u201d.\n\nWill these beneficial capabilities arrive soon enough to help us tackle the biggest challenges? That remains to be seen. But we can try to help improve the odds.\n\nWhat is needed?\nWe can frame this in terms of the things we\u2019re trying to avoid, and/or the things we\u2019re trying to build. Ultimately, we need both, and they support each other \u2014 successfully avoiding perils gives us more time to build the positive things; while in some cases even building early versions of the necessary tech (and getting appropriate adoption) may be a meaningful help in avoiding bad outcomes.\n\nFor avoiding bad outcomes, it\u2019s worth noting that it\u2019s often a better strategy to have a robust response that can intervene early before bad patterns can seriously get going, rather than trying to block them just at the point where things ultimately fall apart. And in an ideal world, there would never be any acute high stakes decisions \u2014 because everything would have enough sanity checks on it that errors of judgement would be caught and corrected.\n\nFor now, though, we\u2019re not super close to that ideal world. Some decisions may be high stakes (most likely those made by major AI companies and/or national security; and most likely by the leadership of each of these), and they may be made well or poorly \u2014 or even without people noticing their importance. One focus, therefore, is to try in relatively direct ways to improve those decisions. Other key focuses are more like \u201ctry to build, and drive adoption of, the kind of tech/structures that we need to get through the crucible in good shape\u201d.\n\nSo \u2026 Key focuses:\n\nKey activities:\n\nBuilding key technologies\n\nDriving adoption of key technologies\n\nShaping the direction of AI technology\n\nTowards safe versions and away from dangerous versions\n\nCould involve:\n\nBasic research\n\nBuilding scientific ~consensus\n\nAdvocating to get decision-makers to adopt safer forms\n\nHelping key decisions to go well\n\nKey technologies:\n\nEpistemic tech to help raise the ceiling \u2014 better understanding of the situation for smart plugged in people\n\nEpistemic tech to help raise the floor \u2014 better understanding of the world for everyone, helping them navigate adversarial informational environments, or process emotional blockers\n\nTech for coordination, to help get off-roads from racing/conflict/disaster\n\nTech for democratic decision-making\n\nTech for democratic oversight, & for avoiding egregiously bad decisions\n\nTech to facilitate better conceptual research \u2014 for boosting alignment, and for avoiding philosophical errors\n\nDirectly defensive tech (e.g. biodefence; cybersecurity; monitoring AI systems for safety)\n\nNB something which I think is important but exclude from this list is \u201ctech to help with general abundance\u201d; while it could help make things easier to navigate in many ways, I think it\u2019s very well incentivized by normal market mechanisms, and is not a strategic priority\n\nNon-tech ways to make key decisions more likely to go well\n\nHelp good people get close to the levers of power\n\nProvide research and advice on what good decisions might look like\n\nHelp to improve decision-making structures\n\nHelp to shape incentives for key actors\n\nMeta / indirect strategies for helping with the above:\n\nField-building\n\nBuilding broad understanding of the strategic situation; especially \u2026\n\n\u2026 of risks, so that people can coordinate to avoid those\n\n\u2026 of the positive futures that are possible, so that people can build towards these\n\n(As I write this list, I\u2019ve a nagging feeling I\u2019m missing some things.)\n\nAgainst an overly-narrow focus\nThese different priorities, to some extent, pull against each other. For example:\n\nIf we are exclusively concerned with loss of control to misaligned AI, the most robust ways to avoid that could involve keeping AI systems tightly contained \u2014 this could prevent or slow broad dissemination of capabilities, which might prevent us from building the radical new technology and social structures that could help us exit the crucible in good shape\n\nIf we are overly focused on preventing misuse of dangerous AI capabilities, we may lean towards damaging hardcore nonproliferation approaches\n\nIf we are exclusively focused on preventing human coups, this could lead us to wanting to radically decentralize power, which could increase misalignment risk\n\nIf we focus just on building positive technology, this could lead to a general policy of trying to accelerate, ignoring the inherent risks\n\nMore broadly, focusing hard on some aspects of the situation is perilous for the usual reasons that maximization is perilous. It makes sense for many individuals or organizations to be focused (because there are efficiency benefits), but as a whole community it\u2019s important to stay in touch with the fact that we almost certainly haven\u2019t traced through all of the important causal pathways involved, and that it\u2019s often a good heuristic to try to make things straightforwardly good in high-leverage domains. (Historically, I think the approach of \u201cthink super hard about the strategic picture, and then soften a bit away from maximization\u201d has outperformed \u201cthink super hard about the strategic picture, and then do the thing that seems highest EV\u201d.)"
  },
  {
    "doc_id": "doc_1",
    "claim_idx": 2,
    "claim": "If developed and deployed in time, AI itself can provide the capabilities needed for safe governance\u2014substantially improving forecasting, information navigation, bargaining/commitment, coordination, and democratic deliberation/oversight\u2014thereby enabling more affluent, stable, and cooperative global politics (\"world peace without world government\").",
    "document_text": "The crucible\nHow I think about the situation with AI\nOwen Cotton-Barratt\nMay 02, 2025\nThe basic situation\nThe world is wild and terrible and wonderful and rushing forwards so so fast.\n\nModern economies are tremendous things, allowing crazy amounts of coordination. People have got really very good at producing stuff. Long-term trends are towards more affluence, and less violence.\n\nThe enlightenment was pretty fantastic not just for bringing us better tech, but also more truthseeking, better values, etc.\n\nPeople, on the whole, are basically good \u2014 they want good things for others, and they want to be liked, and they want the truth to come out. This is some mix of innate and socially conditioned. (It isn\u2019t universal.) But they also often are put in a tight spot and end up looking out for themselves or those they love. The hierarchy of needs bites. Effective altruism often grows from a measure of privilege.\n\nThe world is shaped by economics and by incentives and by institutions and by narratives and by societal values and by moral leadership and by technology. All of these have a complex interplay.\n\nAI enters the picture\n\u201cAI\u201d is a cluster of powerful technologies which are likely to reshape the world. Each of economics and incentives and institutions and narratives and societal values and moral leadership will (I expect) be profoundly impacted by advanced AI. And, of course, AI will be used to shape advanced technologies themselves.\n\nFrom a zoomed-out perspective, there are three impacts of this AI transition which matter most1:\n\nTechnological progress will accelerate \u2014 automation of research means the world will get even faster and crazier\n\nNew kinds of entities could be making the choices that matter \u2014 either purely artificial agents, or new hybrid institutions which incorporate AI deeply in their decision-making\n\nIt will become far easier to centralize control \u2014 a single entity (AI system, or organization, or person) could end up with robust and enduring power over a large domain, or even the entire world\n\nThis is \u2026 kind of wild. 2) and 3) could lead to profound changes to the way the world works, and 1) means this entire thing might happen very quickly (and that people are therefore more likely to fumble it).\n\nFrom the perspective of most people in rich countries today, this is all pretty disturbing. We have enjoyed a good measure of affluence and of stability. This reshaping of the world will bring further affluence, but it is vast and unknown and could easily lead to a collapse of stability.\n\nThe crucible\nHow humanity handles advanced AI is likely to determine the future. We are already entering into a period we could call \u201cthe crucible\u201d \u2014 where AI begins to have a shaping impact on the world. By the time we exit the crucible, some of the broad lines for further unfolding will be sketched out ahead of us\u2026\n\nThe crucible might lead to the ruin of civilization-as-we-know-it:\n\nCatastrophe \u2014\n\nWe might see all-out nuclear war, if there is no agreement when the great powers flex their muscles to exert influence before they become irrelevant\n\nWe might see catastrophic biological attacks, or some other catastrophe caused by future weaponry\n\nLoss of the future \u2014\n\nWe might see a global dictatorship\n\nWe might see the future expropriated from humanity by artificial agents\n\n(Some of these may be recoverable-from; others not. But all seem undesirable \u2014 aside from the straightforward ways in which they\u2019re bad, catastrophes seem more likely to lead to further securitization and consolidation of power, and increase the risk of ultimately ending up with loss of the future, or otherwise poor outcomes from the crucible.)\n\nPart of our role during the crucible will be to steer away from these perils. But this cannot just consist of being cautious and reactive \u2026 the forces that are driving us forwards are powerful and unrelenting. The way out is through \u2014 in order to exit the crucible we must build a system strong enough to contain these forces. Such a system must equip us to recognise and steer away from dangers; and to coordinate actions enough to prevent any unilateral invitations to catastrophe.\n\n\nHeating up\nA reason that I found myself drawn (in writing this) to the crucible metaphor is a sense of things heating up. Right now, AI is starting to be integrated into things, but it\u2019s still kind of small-scale. Most important things are still being done by humans, and by human institutions.\n\nThere will be a period over which things get hotter. AI capabilities will be stronger, and it will take on more important roles. Institutions around AI will become increasingly important. Technological progress will accelerate. Geopolitical tensions are likely to rise.\n\nI\u2019m not sure how fast the crucible will heat up, but I do think it\u2019s unlikely to be instantaneous.\n\nA hotter crucible is by default more dangerous \u2014 the powers at our fingertips become more awesome, and the world seems more fragile in comparison. Misalignment risk will increase. The way through involves using those powers to help contain the danger. If we do not tame these powers, we may see humanity gradually pushed out of the driving seat by the interaction of highly optimized forces.\n\nThe shape of technology\nAI capabilities aren\u2019t a scalar. There are a lot of powerful versions of the technologies that we might develop, and some of these look more concerning or more promising.\n\nTo some extent right now we don\u2019t have a huge ability to steer technology \u2014 competitive pressures push people towards the path of most power soonest. But we do have some ability, and we may well obtain more before we\u2019re through.\n\nWhat kind of shaping are we concerned with, here?\n\nOne important dimension is avoiding dangerous agents. There are various possibilities here. We might strive to avoid powerful agents altogether (AI will still be transformative if we avoid agents!), or to build agentic systems only out of components that are transparent and reliable, or just to avoid especially dangerous approaches like RL-for-agency which may introduce hidden motivations.\n\n[I think there\u2019s something important to say about knowledge; cf. some of my thoughts, and some of Eric Drexler\u2019s; but I\u2019m not in the moment of writing working out how to fit it in.]\n\nOther dimensions are concerned with helping us to navigate the challenges and avoid catastrophe. And above all, to build the kind of positive structures that we will need to safely exit the crucible.\n\nThe case for optimism\nRight now, people are the ones with their hands on the levers, and most people ultimately don\u2019t want the world to go to shit.\n\nThat isn\u2019t enough to stop the world going to shit, of course. But it could be \u2014 if only we were a bit better at working out where things were going, and coming together to work out how to not go in the bad directions.\n\nI don\u2019t want to sound like too much of a naive optimist here. I think that that level of awareness and coordination would be a big reach by the track record of the world at handling major challenges.\n\nBut: what\u2019s holding us back is fundamentally about capabilities. And this is something that AI could be placed to help with.\n\nWith excellent AI tools, we might see:\n\nGreatly increased material affluence, so that it becomes much easier for the world to move out of a default scarcity-mindset\n\nThe best forecasts of the implications of technology being much better, and seen to be much better, than today, so that decision-makers treat them more like common knowledge about the landscape, rather than vague speculation\n\nApplications that help a large number of people fluidly navigate complex informational landscapes, and not get caught up in misinformation (or help them to wisely navigate their own emotional issues, and look more objectively at the world)\n\nDesign of new systems (of many types) which are deeply secure and reliable\n\nBargaining assistance and new commitment solutions that help actors to navigate high-stakes situations without devolving into conflict\n\nMechanisms for democratic deliberation (identifying something like the \u201ccollective will\u201d of a group) and oversight (keeping leaders aligned to the populations they represent)\n\nCoordination mechanisms to allow groups of actors (e.g. AI company researchers) to identify and act in their collective interests\n\nTaken together, these could pave the way to a world with a genuinely healthy political process in any democratic nations which choose to have one, and one in which international politics becomes more civilized and less fraught, as people make agreements (which everyone wants) which robustly take us into Paretotopia rather than risk burning the commons. Perhaps this would lead in the end to something like political unification \u2014 but it could also lead to more of an archipelago of different societies exploring different models, without hurting each other. A slogan might be: \u201cworld peace without world government\u201d.\n\nWill these beneficial capabilities arrive soon enough to help us tackle the biggest challenges? That remains to be seen. But we can try to help improve the odds.\n\nWhat is needed?\nWe can frame this in terms of the things we\u2019re trying to avoid, and/or the things we\u2019re trying to build. Ultimately, we need both, and they support each other \u2014 successfully avoiding perils gives us more time to build the positive things; while in some cases even building early versions of the necessary tech (and getting appropriate adoption) may be a meaningful help in avoiding bad outcomes.\n\nFor avoiding bad outcomes, it\u2019s worth noting that it\u2019s often a better strategy to have a robust response that can intervene early before bad patterns can seriously get going, rather than trying to block them just at the point where things ultimately fall apart. And in an ideal world, there would never be any acute high stakes decisions \u2014 because everything would have enough sanity checks on it that errors of judgement would be caught and corrected.\n\nFor now, though, we\u2019re not super close to that ideal world. Some decisions may be high stakes (most likely those made by major AI companies and/or national security; and most likely by the leadership of each of these), and they may be made well or poorly \u2014 or even without people noticing their importance. One focus, therefore, is to try in relatively direct ways to improve those decisions. Other key focuses are more like \u201ctry to build, and drive adoption of, the kind of tech/structures that we need to get through the crucible in good shape\u201d.\n\nSo \u2026 Key focuses:\n\nKey activities:\n\nBuilding key technologies\n\nDriving adoption of key technologies\n\nShaping the direction of AI technology\n\nTowards safe versions and away from dangerous versions\n\nCould involve:\n\nBasic research\n\nBuilding scientific ~consensus\n\nAdvocating to get decision-makers to adopt safer forms\n\nHelping key decisions to go well\n\nKey technologies:\n\nEpistemic tech to help raise the ceiling \u2014 better understanding of the situation for smart plugged in people\n\nEpistemic tech to help raise the floor \u2014 better understanding of the world for everyone, helping them navigate adversarial informational environments, or process emotional blockers\n\nTech for coordination, to help get off-roads from racing/conflict/disaster\n\nTech for democratic decision-making\n\nTech for democratic oversight, & for avoiding egregiously bad decisions\n\nTech to facilitate better conceptual research \u2014 for boosting alignment, and for avoiding philosophical errors\n\nDirectly defensive tech (e.g. biodefence; cybersecurity; monitoring AI systems for safety)\n\nNB something which I think is important but exclude from this list is \u201ctech to help with general abundance\u201d; while it could help make things easier to navigate in many ways, I think it\u2019s very well incentivized by normal market mechanisms, and is not a strategic priority\n\nNon-tech ways to make key decisions more likely to go well\n\nHelp good people get close to the levers of power\n\nProvide research and advice on what good decisions might look like\n\nHelp to improve decision-making structures\n\nHelp to shape incentives for key actors\n\nMeta / indirect strategies for helping with the above:\n\nField-building\n\nBuilding broad understanding of the strategic situation; especially \u2026\n\n\u2026 of risks, so that people can coordinate to avoid those\n\n\u2026 of the positive futures that are possible, so that people can build towards these\n\n(As I write this list, I\u2019ve a nagging feeling I\u2019m missing some things.)\n\nAgainst an overly-narrow focus\nThese different priorities, to some extent, pull against each other. For example:\n\nIf we are exclusively concerned with loss of control to misaligned AI, the most robust ways to avoid that could involve keeping AI systems tightly contained \u2014 this could prevent or slow broad dissemination of capabilities, which might prevent us from building the radical new technology and social structures that could help us exit the crucible in good shape\n\nIf we are overly focused on preventing misuse of dangerous AI capabilities, we may lean towards damaging hardcore nonproliferation approaches\n\nIf we are exclusively focused on preventing human coups, this could lead us to wanting to radically decentralize power, which could increase misalignment risk\n\nIf we focus just on building positive technology, this could lead to a general policy of trying to accelerate, ignoring the inherent risks\n\nMore broadly, focusing hard on some aspects of the situation is perilous for the usual reasons that maximization is perilous. It makes sense for many individuals or organizations to be focused (because there are efficiency benefits), but as a whole community it\u2019s important to stay in touch with the fact that we almost certainly haven\u2019t traced through all of the important causal pathways involved, and that it\u2019s often a good heuristic to try to make things straightforwardly good in high-leverage domains. (Historically, I think the approach of \u201cthink super hard about the strategic picture, and then soften a bit away from maximization\u201d has outperformed \u201cthink super hard about the strategic picture, and then do the thing that seems highest EV\u201d.)"
  },
  {
    "doc_id": "doc_1",
    "claim_idx": 3,
    "claim": "Navigating the crucible requires a balanced, multi\u2011track global strategy: build and drive adoption of key technologies (epistemic tools that raise both floor and ceiling, coordination mechanisms, democratic decision\u2011making and oversight systems, conceptual\u2011research aids for alignment, and direct defenses like bio/cybersecurity and AI\u2011safety monitoring), steer AI R&D away from dangerous agentic designs toward transparent and reliable systems, and improve early high\u2011stakes decisions by empowering capable, pro\u2011social actors near power\u2014while avoiding single\u2011issue maximization that creates offsetting risks.",
    "document_text": "The crucible\nHow I think about the situation with AI\nOwen Cotton-Barratt\nMay 02, 2025\nThe basic situation\nThe world is wild and terrible and wonderful and rushing forwards so so fast.\n\nModern economies are tremendous things, allowing crazy amounts of coordination. People have got really very good at producing stuff. Long-term trends are towards more affluence, and less violence.\n\nThe enlightenment was pretty fantastic not just for bringing us better tech, but also more truthseeking, better values, etc.\n\nPeople, on the whole, are basically good \u2014 they want good things for others, and they want to be liked, and they want the truth to come out. This is some mix of innate and socially conditioned. (It isn\u2019t universal.) But they also often are put in a tight spot and end up looking out for themselves or those they love. The hierarchy of needs bites. Effective altruism often grows from a measure of privilege.\n\nThe world is shaped by economics and by incentives and by institutions and by narratives and by societal values and by moral leadership and by technology. All of these have a complex interplay.\n\nAI enters the picture\n\u201cAI\u201d is a cluster of powerful technologies which are likely to reshape the world. Each of economics and incentives and institutions and narratives and societal values and moral leadership will (I expect) be profoundly impacted by advanced AI. And, of course, AI will be used to shape advanced technologies themselves.\n\nFrom a zoomed-out perspective, there are three impacts of this AI transition which matter most1:\n\nTechnological progress will accelerate \u2014 automation of research means the world will get even faster and crazier\n\nNew kinds of entities could be making the choices that matter \u2014 either purely artificial agents, or new hybrid institutions which incorporate AI deeply in their decision-making\n\nIt will become far easier to centralize control \u2014 a single entity (AI system, or organization, or person) could end up with robust and enduring power over a large domain, or even the entire world\n\nThis is \u2026 kind of wild. 2) and 3) could lead to profound changes to the way the world works, and 1) means this entire thing might happen very quickly (and that people are therefore more likely to fumble it).\n\nFrom the perspective of most people in rich countries today, this is all pretty disturbing. We have enjoyed a good measure of affluence and of stability. This reshaping of the world will bring further affluence, but it is vast and unknown and could easily lead to a collapse of stability.\n\nThe crucible\nHow humanity handles advanced AI is likely to determine the future. We are already entering into a period we could call \u201cthe crucible\u201d \u2014 where AI begins to have a shaping impact on the world. By the time we exit the crucible, some of the broad lines for further unfolding will be sketched out ahead of us\u2026\n\nThe crucible might lead to the ruin of civilization-as-we-know-it:\n\nCatastrophe \u2014\n\nWe might see all-out nuclear war, if there is no agreement when the great powers flex their muscles to exert influence before they become irrelevant\n\nWe might see catastrophic biological attacks, or some other catastrophe caused by future weaponry\n\nLoss of the future \u2014\n\nWe might see a global dictatorship\n\nWe might see the future expropriated from humanity by artificial agents\n\n(Some of these may be recoverable-from; others not. But all seem undesirable \u2014 aside from the straightforward ways in which they\u2019re bad, catastrophes seem more likely to lead to further securitization and consolidation of power, and increase the risk of ultimately ending up with loss of the future, or otherwise poor outcomes from the crucible.)\n\nPart of our role during the crucible will be to steer away from these perils. But this cannot just consist of being cautious and reactive \u2026 the forces that are driving us forwards are powerful and unrelenting. The way out is through \u2014 in order to exit the crucible we must build a system strong enough to contain these forces. Such a system must equip us to recognise and steer away from dangers; and to coordinate actions enough to prevent any unilateral invitations to catastrophe.\n\n\nHeating up\nA reason that I found myself drawn (in writing this) to the crucible metaphor is a sense of things heating up. Right now, AI is starting to be integrated into things, but it\u2019s still kind of small-scale. Most important things are still being done by humans, and by human institutions.\n\nThere will be a period over which things get hotter. AI capabilities will be stronger, and it will take on more important roles. Institutions around AI will become increasingly important. Technological progress will accelerate. Geopolitical tensions are likely to rise.\n\nI\u2019m not sure how fast the crucible will heat up, but I do think it\u2019s unlikely to be instantaneous.\n\nA hotter crucible is by default more dangerous \u2014 the powers at our fingertips become more awesome, and the world seems more fragile in comparison. Misalignment risk will increase. The way through involves using those powers to help contain the danger. If we do not tame these powers, we may see humanity gradually pushed out of the driving seat by the interaction of highly optimized forces.\n\nThe shape of technology\nAI capabilities aren\u2019t a scalar. There are a lot of powerful versions of the technologies that we might develop, and some of these look more concerning or more promising.\n\nTo some extent right now we don\u2019t have a huge ability to steer technology \u2014 competitive pressures push people towards the path of most power soonest. But we do have some ability, and we may well obtain more before we\u2019re through.\n\nWhat kind of shaping are we concerned with, here?\n\nOne important dimension is avoiding dangerous agents. There are various possibilities here. We might strive to avoid powerful agents altogether (AI will still be transformative if we avoid agents!), or to build agentic systems only out of components that are transparent and reliable, or just to avoid especially dangerous approaches like RL-for-agency which may introduce hidden motivations.\n\n[I think there\u2019s something important to say about knowledge; cf. some of my thoughts, and some of Eric Drexler\u2019s; but I\u2019m not in the moment of writing working out how to fit it in.]\n\nOther dimensions are concerned with helping us to navigate the challenges and avoid catastrophe. And above all, to build the kind of positive structures that we will need to safely exit the crucible.\n\nThe case for optimism\nRight now, people are the ones with their hands on the levers, and most people ultimately don\u2019t want the world to go to shit.\n\nThat isn\u2019t enough to stop the world going to shit, of course. But it could be \u2014 if only we were a bit better at working out where things were going, and coming together to work out how to not go in the bad directions.\n\nI don\u2019t want to sound like too much of a naive optimist here. I think that that level of awareness and coordination would be a big reach by the track record of the world at handling major challenges.\n\nBut: what\u2019s holding us back is fundamentally about capabilities. And this is something that AI could be placed to help with.\n\nWith excellent AI tools, we might see:\n\nGreatly increased material affluence, so that it becomes much easier for the world to move out of a default scarcity-mindset\n\nThe best forecasts of the implications of technology being much better, and seen to be much better, than today, so that decision-makers treat them more like common knowledge about the landscape, rather than vague speculation\n\nApplications that help a large number of people fluidly navigate complex informational landscapes, and not get caught up in misinformation (or help them to wisely navigate their own emotional issues, and look more objectively at the world)\n\nDesign of new systems (of many types) which are deeply secure and reliable\n\nBargaining assistance and new commitment solutions that help actors to navigate high-stakes situations without devolving into conflict\n\nMechanisms for democratic deliberation (identifying something like the \u201ccollective will\u201d of a group) and oversight (keeping leaders aligned to the populations they represent)\n\nCoordination mechanisms to allow groups of actors (e.g. AI company researchers) to identify and act in their collective interests\n\nTaken together, these could pave the way to a world with a genuinely healthy political process in any democratic nations which choose to have one, and one in which international politics becomes more civilized and less fraught, as people make agreements (which everyone wants) which robustly take us into Paretotopia rather than risk burning the commons. Perhaps this would lead in the end to something like political unification \u2014 but it could also lead to more of an archipelago of different societies exploring different models, without hurting each other. A slogan might be: \u201cworld peace without world government\u201d.\n\nWill these beneficial capabilities arrive soon enough to help us tackle the biggest challenges? That remains to be seen. But we can try to help improve the odds.\n\nWhat is needed?\nWe can frame this in terms of the things we\u2019re trying to avoid, and/or the things we\u2019re trying to build. Ultimately, we need both, and they support each other \u2014 successfully avoiding perils gives us more time to build the positive things; while in some cases even building early versions of the necessary tech (and getting appropriate adoption) may be a meaningful help in avoiding bad outcomes.\n\nFor avoiding bad outcomes, it\u2019s worth noting that it\u2019s often a better strategy to have a robust response that can intervene early before bad patterns can seriously get going, rather than trying to block them just at the point where things ultimately fall apart. And in an ideal world, there would never be any acute high stakes decisions \u2014 because everything would have enough sanity checks on it that errors of judgement would be caught and corrected.\n\nFor now, though, we\u2019re not super close to that ideal world. Some decisions may be high stakes (most likely those made by major AI companies and/or national security; and most likely by the leadership of each of these), and they may be made well or poorly \u2014 or even without people noticing their importance. One focus, therefore, is to try in relatively direct ways to improve those decisions. Other key focuses are more like \u201ctry to build, and drive adoption of, the kind of tech/structures that we need to get through the crucible in good shape\u201d.\n\nSo \u2026 Key focuses:\n\nKey activities:\n\nBuilding key technologies\n\nDriving adoption of key technologies\n\nShaping the direction of AI technology\n\nTowards safe versions and away from dangerous versions\n\nCould involve:\n\nBasic research\n\nBuilding scientific ~consensus\n\nAdvocating to get decision-makers to adopt safer forms\n\nHelping key decisions to go well\n\nKey technologies:\n\nEpistemic tech to help raise the ceiling \u2014 better understanding of the situation for smart plugged in people\n\nEpistemic tech to help raise the floor \u2014 better understanding of the world for everyone, helping them navigate adversarial informational environments, or process emotional blockers\n\nTech for coordination, to help get off-roads from racing/conflict/disaster\n\nTech for democratic decision-making\n\nTech for democratic oversight, & for avoiding egregiously bad decisions\n\nTech to facilitate better conceptual research \u2014 for boosting alignment, and for avoiding philosophical errors\n\nDirectly defensive tech (e.g. biodefence; cybersecurity; monitoring AI systems for safety)\n\nNB something which I think is important but exclude from this list is \u201ctech to help with general abundance\u201d; while it could help make things easier to navigate in many ways, I think it\u2019s very well incentivized by normal market mechanisms, and is not a strategic priority\n\nNon-tech ways to make key decisions more likely to go well\n\nHelp good people get close to the levers of power\n\nProvide research and advice on what good decisions might look like\n\nHelp to improve decision-making structures\n\nHelp to shape incentives for key actors\n\nMeta / indirect strategies for helping with the above:\n\nField-building\n\nBuilding broad understanding of the strategic situation; especially \u2026\n\n\u2026 of risks, so that people can coordinate to avoid those\n\n\u2026 of the positive futures that are possible, so that people can build towards these\n\n(As I write this list, I\u2019ve a nagging feeling I\u2019m missing some things.)\n\nAgainst an overly-narrow focus\nThese different priorities, to some extent, pull against each other. For example:\n\nIf we are exclusively concerned with loss of control to misaligned AI, the most robust ways to avoid that could involve keeping AI systems tightly contained \u2014 this could prevent or slow broad dissemination of capabilities, which might prevent us from building the radical new technology and social structures that could help us exit the crucible in good shape\n\nIf we are overly focused on preventing misuse of dangerous AI capabilities, we may lean towards damaging hardcore nonproliferation approaches\n\nIf we are exclusively focused on preventing human coups, this could lead us to wanting to radically decentralize power, which could increase misalignment risk\n\nIf we focus just on building positive technology, this could lead to a general policy of trying to accelerate, ignoring the inherent risks\n\nMore broadly, focusing hard on some aspects of the situation is perilous for the usual reasons that maximization is perilous. It makes sense for many individuals or organizations to be focused (because there are efficiency benefits), but as a whole community it\u2019s important to stay in touch with the fact that we almost certainly haven\u2019t traced through all of the important causal pathways involved, and that it\u2019s often a good heuristic to try to make things straightforwardly good in high-leverage domains. (Historically, I think the approach of \u201cthink super hard about the strategic picture, and then soften a bit away from maximization\u201d has outperformed \u201cthink super hard about the strategic picture, and then do the thing that seems highest EV\u201d.)"
  }
]